{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì Learning GenAI: Building a Semantic Search Engine\n",
        "## *From GCP Data Engineer to GenAI Developer*\n",
        "\n",
        "Welcome to your GenAI learning journey! This notebook will teach you how to build a **Semantic Search Engine** step by step.\n",
        "\n",
        "### üß† **What You'll Learn:**\n",
        "1. **RAG (Retrieval-Augmented Generation)** - How to make AI smarter with your own data\n",
        "2. **Vector Embeddings** - Converting text into mathematical representations\n",
        "3. **Semantic Search** - Finding meaning, not just keywords\n",
        "4. **LangChain Framework** - The toolkit for GenAI applications\n",
        "5. **Document Processing** - From raw files to AI-ready chunks\n",
        "\n",
        "### üèóÔ∏è **What We're Building:**\n",
        "A system that can:\n",
        "- üìö Read your documents from GCS (like your data pipelines!)\n",
        "- üßÆ Convert text into vectors (embeddings)\n",
        "- üîç Find relevant information semantically \n",
        "- ü§ñ Answer questions using Gemini 2.5 Pro\n",
        "- üìù Show sources (like good data lineage!)\n",
        "\n",
        "### üí° **Key Concepts We'll Explore:**\n",
        "- **Embeddings**: Text ‚Üí Numbers that capture meaning\n",
        "- **Vector Database**: Storage optimized for similarity search\n",
        "- **Chunking**: Breaking documents into digestible pieces\n",
        "- **Retrieval**: Finding relevant context for questions\n",
        "- **Generation**: Using LLM to create answers from context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Understanding & Installing Dependencies\n",
        "\n",
        "Before we build our GenAI application, let's understand what each tool does:\n",
        "\n",
        "### üîß **Core GenAI Stack:**\n",
        "- **`langchain`**: The main framework - think of it as pandas for GenAI\n",
        "- **`langchain-google-vertexai`**: Connects LangChain to Google's AI models\n",
        "- **`sentence-transformers`**: Creates embeddings (text ‚Üí vectors)\n",
        "- **`chromadb`**: Vector database for similarity search\n",
        "- **`PyPDF2`, `python-docx`**: Document parsers (like reading CSV/JSON files)\n",
        "\n",
        "### ü§î **Why These Specific Packages?**\n",
        "- **LangChain**: Simplifies complex GenAI workflows (like Apache Beam for data)\n",
        "- **Embeddings**: Convert text to numbers so computers can understand similarity\n",
        "- **Vector DB**: Specialized storage for finding \"similar\" items quickly\n",
        "- **Document Loaders**: Handle different file formats automatically\n",
        "\n",
        "Let's install everything with compatible versions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Installing packages for our GenAI application...\n",
            "This is like setting up your data engineering environment, but for AI!\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "‚úÖ Installation complete!\n",
            "üéâ You now have a complete GenAI development environment!\n",
            "\n",
            "üìö What we just installed:\n",
            "   ‚Ä¢ LangChain: GenAI application framework\n",
            "   ‚Ä¢ ChromaDB: Vector database for semantic search\n",
            "   ‚Ä¢ Sentence Transformers: Text embedding models\n",
            "   ‚Ä¢ Document parsers: PDF, DOCX, TXT support\n",
            "   ‚Ä¢ GCS integration: Connect to your cloud storage\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Installing the GenAI Toolkit\n",
        "print(\"üîß Installing packages for our GenAI application...\")\n",
        "print(\"This is like setting up your data engineering environment, but for AI!\")\n",
        "\n",
        "# Core LangChain framework and Google integrations\n",
        "%pip install -qU langchain langchain-google-vertexai langchain-community langchain-text-splitters\n",
        "\n",
        "# Vector storage and embeddings\n",
        "%pip install -qU \"google-cloud-storage<3.0.0,>=2.18.0\" chromadb sentence-transformers\n",
        "\n",
        "# Document processing utilities\n",
        "%pip install -qU python-dotenv PyPDF2 python-docx unstructured tiktoken faiss-cpu\n",
        "\n",
        "print(\"\\n‚úÖ Installation complete!\")\n",
        "print(\"üéâ You now have a complete GenAI development environment!\")\n",
        "print(\"\\nüìö What we just installed:\")\n",
        "print(\"   ‚Ä¢ LangChain: GenAI application framework\")\n",
        "print(\"   ‚Ä¢ ChromaDB: Vector database for semantic search\") \n",
        "print(\"   ‚Ä¢ Sentence Transformers: Text embedding models\")\n",
        "print(\"   ‚Ä¢ Document parsers: PDF, DOCX, TXT support\")\n",
        "print(\"   ‚Ä¢ GCS integration: Connect to your cloud storage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Importing libraries - let me explain what each does...\n",
            "‚úÖ Standard Python utilities imported\n",
            "\n",
            "ü¶ú Importing LangChain components:\n",
            "   ‚Ä¢ init_chat_model: Connects to Gemini 2.5 Pro\n",
            "   ‚Ä¢ Document loaders: Read PDF, DOCX, TXT files\n",
            "   ‚Ä¢ Text splitter: Breaks documents into chunks\n",
            "   ‚Ä¢ Embeddings: Convert text to vectors\n",
            "   ‚Ä¢ ChromaDB: Vector database for similarity search\n",
            "   ‚Ä¢ RetrievalQA: Combines search + question answering\n",
            "   ‚Ä¢ PromptTemplate: Structure how we ask the AI\n",
            "\n",
            "‚òÅÔ∏è Google Cloud components:\n",
            "   ‚Ä¢ GCS Storage: Access your cloud documents\n",
            "\n",
            "üéâ All imports successful!\n",
            "üí° Think of LangChain as your 'pandas for GenAI' - it handles the complex stuff!\n"
          ]
        }
      ],
      "source": [
        "# üìö Importing Our GenAI Toolkit\n",
        "print(\"üìö Importing libraries - let me explain what each does...\")\n",
        "\n",
        "# Standard Python libraries (you know these!)\n",
        "import os\n",
        "import tempfile\n",
        "from typing import List, Dict, Any\n",
        "from pathlib import Path\n",
        "print(\"‚úÖ Standard Python utilities imported\")\n",
        "\n",
        "# ü¶ú LangChain Core Components\n",
        "print(\"\\nü¶ú Importing LangChain components:\")\n",
        "from langchain.chat_models import init_chat_model  # Initialize AI models\n",
        "print(\"   ‚Ä¢ init_chat_model: Connects to Gemini 2.5 Pro\")\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "print(\"   ‚Ä¢ Document loaders: Read PDF, DOCX, TXT files\")\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "print(\"   ‚Ä¢ Text splitter: Breaks documents into chunks\")\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "print(\"   ‚Ä¢ Embeddings: Convert text to vectors\")\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "print(\"   ‚Ä¢ ChromaDB: Vector database for similarity search\")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "print(\"   ‚Ä¢ RetrievalQA: Combines search + question answering\")\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "print(\"   ‚Ä¢ PromptTemplate: Structure how we ask the AI\")\n",
        "\n",
        "# ‚òÅÔ∏è Google Cloud Integration\n",
        "print(\"\\n‚òÅÔ∏è Google Cloud components:\")\n",
        "from google.cloud import storage\n",
        "print(\"   ‚Ä¢ GCS Storage: Access your cloud documents\")\n",
        "\n",
        "# Clean up warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\nüéâ All imports successful!\")\n",
        "print(\"üí° Think of LangChain as your 'pandas for GenAI' - it handles the complex stuff!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Step 2: Configuration - Setting Up Your GenAI Pipeline\n",
        "\n",
        "Just like configuring a data pipeline, we need to set parameters for our GenAI application.\n",
        "\n",
        "### üéØ **Configuration Concepts:**\n",
        "- **Chunk Size**: How big should text pieces be? (like batch size in data processing)\n",
        "- **Overlap**: How much text to share between chunks (ensures continuity)\n",
        "- **Model Temperature**: How creative should AI responses be? (0=precise, 1=creative)\n",
        "- **Similarity Search K**: How many relevant documents to retrieve?\n",
        "\n",
        "### üîß **Think of This As:**\n",
        "- Spark configuration for distributed processing\n",
        "- Database connection settings  \n",
        "- ETL pipeline parameters\n",
        "\n",
        "Let's configure our system:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è Setting up configuration - like your data pipeline configs!\n",
            "üèóÔ∏è Infrastructure configuration...\n",
            "ü§ñ AI model configuration...\n",
            "üìÑ Document processing configuration...\n",
            "üóÑÔ∏è Vector storage configuration...\n",
            "üßÆ Embedding model configuration...\n",
            "üîç Search configuration...\n",
            "\n",
            "‚úÖ Configuration Complete!\n",
            "üìä Configuration Summary:\n",
            "   ü™£ Data Source: genai-sai-bucket\n",
            "   ü§ñ AI Model: gemini-2.5-pro\n",
            "   üßÆ Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   üìè Chunk Size: 1000 characters\n",
            "   üîç Retrieval Count: 4 documents\n",
            "\n",
            "üí° Pro Tip: These settings affect performance and accuracy!\n",
            "   ‚Ä¢ Larger chunks = more context but slower processing\n",
            "   ‚Ä¢ Higher K = more context but potential noise\n",
            "   ‚Ä¢ Temperature 0 = factual, higher = creative\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è GenAI Configuration Class\n",
        "print(\"‚öôÔ∏è Setting up configuration - like your data pipeline configs!\")\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration for our GenAI Semantic Search Engine\"\"\"\n",
        "    \n",
        "    # üèóÔ∏è Infrastructure Settings (like your GCP project setup)\n",
        "    print(\"üèóÔ∏è Infrastructure configuration...\")\n",
        "    GCS_BUCKET_NAME = \"genai-sai-bucket\"  # ü™£ Your data storage\n",
        "    GOOGLE_CLOUD_PROJECT = \"igneous-future-451513-v8d\"  # üåç Your GCP project\n",
        "    GCS_FOLDER_PATH = \"\"  # üìÅ Optional: specific folder (like partitions!)\n",
        "    \n",
        "    # ü§ñ AI Model Settings\n",
        "    print(\"ü§ñ AI model configuration...\")\n",
        "    MODEL_NAME = \"gemini-2.5-pro\"  # The LLM brain\n",
        "    MODEL_PROVIDER = \"google_vertexai\"  # Google's AI platform\n",
        "    TEMPERATURE = 0.0  # üå°Ô∏è 0=factual, 1=creative (like randomness in ML)\n",
        "    \n",
        "    # üìÑ Document Processing (like ETL transformations)\n",
        "    print(\"üìÑ Document processing configuration...\")\n",
        "    CHUNK_SIZE = 1000  # üìè Characters per chunk (like batch size)\n",
        "    CHUNK_OVERLAP = 200  # üîÑ Overlap between chunks (ensures continuity)\n",
        "    \n",
        "    # üóÑÔ∏è Vector Database Settings (like your data warehouse config)\n",
        "    print(\"üóÑÔ∏è Vector storage configuration...\")\n",
        "    VECTOR_STORE_PERSIST_DIRECTORY = \"./vector_store\"  # üíæ Local storage\n",
        "    VECTOR_STORE_COLLECTION_NAME = \"documents\"  # üè∑Ô∏è Table name equivalent\n",
        "    \n",
        "    # üßÆ Embedding Model (converts text ‚Üí numbers)\n",
        "    print(\"üßÆ Embedding model configuration...\")\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight & fast\n",
        "    \n",
        "    # üîç Search Parameters\n",
        "    print(\"üîç Search configuration...\")\n",
        "    SIMILARITY_SEARCH_K = 4  # How many relevant docs to retrieve\n",
        "    SIMILARITY_SCORE_THRESHOLD = 0.7  # Minimum relevance score\n",
        "\n",
        "# Initialize our configuration\n",
        "config = Config()\n",
        "\n",
        "print(\"\\n‚úÖ Configuration Complete!\")\n",
        "print(f\"üìä Configuration Summary:\")\n",
        "print(f\"   ü™£ Data Source: {config.GCS_BUCKET_NAME}\")\n",
        "print(f\"   ü§ñ AI Model: {config.MODEL_NAME}\")\n",
        "print(f\"   üßÆ Embedding Model: {config.EMBEDDING_MODEL}\")\n",
        "print(f\"   üìè Chunk Size: {config.CHUNK_SIZE} characters\")\n",
        "print(f\"   üîç Retrieval Count: {config.SIMILARITY_SEARCH_K} documents\")\n",
        "\n",
        "print(f\"\\nüí° Pro Tip: These settings affect performance and accuracy!\")\n",
        "print(f\"   ‚Ä¢ Larger chunks = more context but slower processing\")\n",
        "print(f\"   ‚Ä¢ Higher K = more context but potential noise\")\n",
        "print(f\"   ‚Ä¢ Temperature 0 = factual, higher = creative\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Step 3: Building a Document Loader - Your First GenAI Component\n",
        "\n",
        "Let's build our first GenAI component! This is like creating a data ingestion pipeline, but for AI.\n",
        "\n",
        "### üîç **What This Component Does:**\n",
        "1. **Connects to GCS** (like connecting to BigQuery)\n",
        "2. **Lists available documents** (like scanning table partitions)\n",
        "3. **Downloads files to local temp storage** (like caching data)\n",
        "4. **Parses different formats** (PDF, DOCX, TXT - like handling CSV, JSON, Parquet)\n",
        "5. **Adds metadata** (source tracking - like data lineage!)\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand document ingestion patterns in GenAI\n",
        "- See how LangChain handles different file formats\n",
        "- Learn about temporary file management\n",
        "- Practice error handling in GenAI pipelines\n",
        "\n",
        "### üí≠ **Data Engineering Parallels:**\n",
        "- This is like an ETL extract phase\n",
        "- File format handling = data schema validation\n",
        "- Metadata addition = data cataloging\n",
        "- Error handling = data quality checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è Creating our first GenAI component - Document Loader!\n",
            "This is like building a data ingestion pipeline for AI...\n",
            "‚úÖ GCS Document Loader class ready!\n",
            "üí° This is your 'Extract' component in the GenAI ETL pipeline\n",
            "üéØ Next: We'll build the 'Transform' and 'Load' components!\n"
          ]
        }
      ],
      "source": [
        "# üìö Building Our Document Loader Class\n",
        "print(\"üèóÔ∏è Creating our first GenAI component - Document Loader!\")\n",
        "print(\"This is like building a data ingestion pipeline for AI...\")\n",
        "\n",
        "class GCSDocumentLoader:\n",
        "    \"\"\"\n",
        "    üìö Document Loader - Your GenAI Data Ingestion Pipeline\n",
        "    \n",
        "    This class handles the 'Extract' part of our GenAI ETL process:\n",
        "    ‚Ä¢ Connects to GCS (your data lake)\n",
        "    ‚Ä¢ Downloads documents (data ingestion)\n",
        "    ‚Ä¢ Parses different formats (schema handling)\n",
        "    ‚Ä¢ Adds metadata (data lineage)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bucket_name: str, project_id: str, folder_path: str = \"\"):\n",
        "        print(f\"üîß Initializing Document Loader...\")\n",
        "        self.bucket_name = bucket_name\n",
        "        self.project_id = project_id\n",
        "        self.folder_path = folder_path\n",
        "        self.client = None  # Will be initialized when needed\n",
        "        print(f\"   üìç Target: gs://{bucket_name}/{folder_path}\")\n",
        "        \n",
        "    def _initialize_client(self):\n",
        "        \"\"\"üîë Initialize GCS client - like connecting to your data warehouse\"\"\"\n",
        "        print(\"üîë Connecting to Google Cloud Storage...\")\n",
        "        try:\n",
        "            # üóùÔ∏è Use service account authentication (secure!)\n",
        "            credentials_path = \"genai-sai-auth.json\"\n",
        "            print(f\"   üìã Using credentials: {credentials_path}\")\n",
        "            \n",
        "            from google.oauth2 import service_account\n",
        "            credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
        "            \n",
        "            # Initialize the GCS client\n",
        "            self.client = storage.Client(project=self.project_id, credentials=credentials)\n",
        "            print(f\"‚úÖ Connected to GCS project: {self.project_id}\")\n",
        "            print(f\"üîê Authentication successful!\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Connection failed: {str(e)}\")\n",
        "            print(\"üí° Check: genai-sai-auth.json exists and has proper permissions\")\n",
        "            raise\n",
        "    \n",
        "    def list_documents(self) -> List[str]:\n",
        "        \"\"\"üìã Discover available documents - like scanning your data catalog\"\"\"\n",
        "        print(\"üìã Scanning for documents in GCS bucket...\")\n",
        "        \n",
        "        if not self.client:\n",
        "            self._initialize_client()\n",
        "        \n",
        "        assert self.client is not None, \"Failed to initialize GCS client\"\n",
        "        \n",
        "        # ü™£ Access the bucket (like connecting to a database)\n",
        "        bucket = self.client.bucket(self.bucket_name)\n",
        "        blobs = bucket.list_blobs(prefix=self.folder_path)\n",
        "        \n",
        "        # üîç Filter for supported document types\n",
        "        document_files = []\n",
        "        supported_extensions = ['.pdf', '.docx', '.txt']\n",
        "        print(f\"   üéØ Looking for: {', '.join(supported_extensions)} files\")\n",
        "        \n",
        "        for blob in blobs:\n",
        "            if any(blob.name.lower().endswith(ext) for ext in supported_extensions):\n",
        "                document_files.append(blob.name)\n",
        "                print(f\"   üìÑ Found: {blob.name}\")\n",
        "        \n",
        "        print(f\"‚úÖ Discovery complete: {len(document_files)} documents found\")\n",
        "        return document_files\n",
        "    \n",
        "    def download_and_load_documents(self) -> List:\n",
        "        \"\"\"\n",
        "        üì• Download & Parse Documents - The core ingestion process\n",
        "        \n",
        "        This is like your ETL extract + transform phases:\n",
        "        1. Download from cloud storage\n",
        "        2. Parse different file formats  \n",
        "        3. Extract text content\n",
        "        4. Add metadata for tracking\n",
        "        \"\"\"\n",
        "        print(\"üì• Starting document ingestion pipeline...\")\n",
        "        \n",
        "        if not self.client:\n",
        "            self._initialize_client()\n",
        "        \n",
        "        assert self.client is not None, \"Failed to initialize GCS client\"\n",
        "        \n",
        "        # Initialize containers\n",
        "        documents = []\n",
        "        bucket = self.client.bucket(self.bucket_name)\n",
        "        document_files = self.list_documents()\n",
        "        \n",
        "        if not document_files:\n",
        "            print(\"‚ö†Ô∏è No documents found to process!\")\n",
        "            return documents\n",
        "            \n",
        "        print(f\"üöÄ Processing {len(document_files)} documents...\")\n",
        "        \n",
        "        # Process each document\n",
        "        for i, file_name in enumerate(document_files, 1):\n",
        "            print(f\"\\nüìÑ Processing {i}/{len(document_files)}: {file_name}\")\n",
        "            \n",
        "            try:\n",
        "                # üíæ Create temporary file (like staging area in ETL)\n",
        "                file_suffix = Path(file_name).suffix\n",
        "                print(f\"   üìÅ Creating temp file with suffix: {file_suffix}\")\n",
        "                \n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=file_suffix) as temp_file:\n",
        "                    # ‚¨áÔ∏è Download from GCS\n",
        "                    print(f\"   ‚¨áÔ∏è Downloading from GCS...\")\n",
        "                    blob = bucket.blob(file_name)\n",
        "                    blob.download_to_filename(temp_file.name)\n",
        "                    print(f\"   ‚úÖ Downloaded to: {temp_file.name}\")\n",
        "                    \n",
        "                    # üîß Choose appropriate parser based on file type\n",
        "                    print(f\"   üîß Selecting parser for: {file_suffix}\")\n",
        "                    if file_name.lower().endswith('.pdf'):\n",
        "                        loader = PyPDFLoader(temp_file.name)\n",
        "                        print(\"   üìñ Using PDF parser\")\n",
        "                    elif file_name.lower().endswith('.docx'):\n",
        "                        loader = Docx2txtLoader(temp_file.name)\n",
        "                        print(\"   üìù Using DOCX parser\")\n",
        "                    elif file_name.lower().endswith('.txt'):\n",
        "                        loader = TextLoader(temp_file.name)\n",
        "                        print(\"   üìÑ Using TXT parser\")\n",
        "                    else:\n",
        "                        print(f\"   ‚ö†Ô∏è Unsupported file type: {file_suffix}\")\n",
        "                        continue\n",
        "                    \n",
        "                    # üìñ Parse the document content\n",
        "                    print(\"   üìñ Parsing document content...\")\n",
        "                    doc_content = loader.load()\n",
        "                    print(f\"   üìä Extracted {len(doc_content)} pages/sections\")\n",
        "                    \n",
        "                    # üè∑Ô∏è Add metadata (like data lineage!)\n",
        "                    print(\"   üè∑Ô∏è Adding metadata...\")\n",
        "                    for doc in doc_content:\n",
        "                        doc.metadata['source'] = file_name  # Original filename\n",
        "                        doc.metadata['bucket'] = self.bucket_name  # Source bucket\n",
        "                        doc.metadata['file_type'] = file_suffix  # File format\n",
        "                        doc.metadata['processed_at'] = str(os.path.getmtime(temp_file.name))\n",
        "                    \n",
        "                    documents.extend(doc_content)\n",
        "                    print(f\"   ‚úÖ Successfully processed: {file_name}\")\n",
        "                    \n",
        "                # üßπ Clean up temporary file (good housekeeping!)\n",
        "                os.unlink(temp_file.name)\n",
        "                print(f\"   üßπ Cleaned up temp file\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error processing {file_name}: {str(e)}\")\n",
        "                print(f\"   üîÑ Continuing with next file...\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\nüéâ Ingestion Complete!\")\n",
        "        print(f\"   üìä Total documents processed: {len(document_files)}\")\n",
        "        print(f\"   üìÑ Total text chunks extracted: {len(documents)}\")\n",
        "        print(f\"   üéØ Ready for the next pipeline stage!\")\n",
        "        \n",
        "        return documents\n",
        "\n",
        "print(\"‚úÖ GCS Document Loader class ready!\")\n",
        "print(\"üí° This is your 'Extract' component in the GenAI ETL pipeline\")\n",
        "print(\"üéØ Next: We'll build the 'Transform' and 'Load' components!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEST \n",
        "# config = Config()\n",
        "# gcsloader = GCSDocumentLoader(bucket_name=config.GCS_BUCKET_NAME, project_id=config.GOOGLE_CLOUD_PROJECT, folder_path=config.GCS_FOLDER_PATH)\n",
        "# documents = gcsloader.download_and_load_documents()\n",
        "# print(f\"‚úÖ Documents loaded: {len(documents)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßÆ Step 4: The Magic of Embeddings & Vector Search - The Heart of GenAI\n",
        "\n",
        "Now we're getting to the exciting part! Let's understand how AI actually \"understands\" your documents.\n",
        "\n",
        "### üß† **Core Concepts to Learn:**\n",
        "\n",
        "#### üìè **Text Embeddings (Text ‚Üí Numbers):**\n",
        "- Convert words to vectors: \"Hello\" ‚Üí [0.1, 0.8, -0.3, ...]\n",
        "- Similar words have similar vectors\n",
        "- Like GPS coordinates, but for meaning!\n",
        "\n",
        "#### üîç **Vector Database:**\n",
        "- Stores these number representations\n",
        "- Finds \"similar\" vectors super fast\n",
        "- Like a search index, but for meaning instead of exact words\n",
        "\n",
        "#### ‚úÇÔ∏è **Document Chunking:**\n",
        "- Split long documents into smaller pieces\n",
        "- Balance: Big chunks = more context, Small chunks = precise answers\n",
        "- Like pagination in databases\n",
        "\n",
        "### üéØ **The RAG (Retrieval-Augmented Generation) Pattern:**\n",
        "1. **Split** documents into chunks\n",
        "2. **Convert** chunks to embeddings (vectors)\n",
        "3. **Store** in vector database\n",
        "4. **Search** for relevant chunks when user asks question\n",
        "5. **Generate** answer using retrieved context + LLM\n",
        "\n",
        "Let's build this step by step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† Building the Semantic Search Engine - This is where the magic happens!\n",
            "We're creating the 'Transform' and 'Load' parts of our GenAI ETL pipeline...\n",
            "‚úÖ Semantic Search Engine class complete!\n",
            "üéâ You've just built a complete RAG (Retrieval-Augmented Generation) system!\n",
            "üí° This combines the best of search engines and AI language models\n",
            "üéØ Next: Let's put it all together and test it!\n"
          ]
        }
      ],
      "source": [
        "# üßÆ Building the Semantic Search Engine - The Brain of Our System\n",
        "print(\"üß† Building the Semantic Search Engine - This is where the magic happens!\")\n",
        "print(\"We're creating the 'Transform' and 'Load' parts of our GenAI ETL pipeline...\")\n",
        "\n",
        "class SemanticSearchEngine:\n",
        "    \"\"\"\n",
        "    üßÆ Semantic Search Engine - The GenAI Processing Brain\n",
        "    \n",
        "    This class handles the core GenAI transformations:\n",
        "    ‚Ä¢ Text chunking (data preprocessing)\n",
        "    ‚Ä¢ Embeddings generation (feature engineering) \n",
        "    ‚Ä¢ Vector storage (specialized database)\n",
        "    ‚Ä¢ Similarity search (intelligent querying)\n",
        "    ‚Ä¢ Question answering (the final output)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        print(\"üîß Initializing Semantic Search Engine...\")\n",
        "        self.config = config\n",
        "        \n",
        "        # Initialize all components as None (lazy loading)\n",
        "        self.embeddings = None      # Text ‚Üí Vector converter\n",
        "        self.vector_store = None    # Vector database\n",
        "        self.retriever = None       # Search interface\n",
        "        self.model = None          # LLM (Gemini 2.5 Pro)\n",
        "        self.qa_chain = None       # Complete QA pipeline\n",
        "        \n",
        "        print(\"   üìä Engine initialized with lazy loading pattern\")\n",
        "        print(\"   üéØ Components will be created when needed\")\n",
        "        \n",
        "    def initialize_embeddings(self):\n",
        "        \"\"\"\n",
        "        üßÆ Initialize Embedding Model - Converting Text to Vectors\n",
        "        \n",
        "        This is like feature engineering in ML:\n",
        "        ‚Ä¢ Converts text to numerical vectors\n",
        "        ‚Ä¢ Captures semantic meaning\n",
        "        ‚Ä¢ Enables similarity calculations\n",
        "        \"\"\"\n",
        "        print(\"üßÆ Setting up embedding model...\")\n",
        "        print(\"   üìê This converts text to vectors (numbers that capture meaning)\")\n",
        "        \n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=self.config.EMBEDDING_MODEL,\n",
        "            model_kwargs={'device': 'cpu'}  # Use CPU for compatibility\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Embedding model ready: {self.config.EMBEDDING_MODEL}\")\n",
        "        print(\"   üí° Now we can convert any text to vectors!\")\n",
        "        print(\"   üìä Example: 'Hello' ‚Üí [0.1, 0.8, -0.3, ...384 numbers]\")\n",
        "    \n",
        "    def process_documents(self, documents):\n",
        "        \"\"\"\n",
        "        ‚úÇÔ∏è Document Chunking - Breaking Text into Digestible Pieces\n",
        "        \n",
        "        This is like data preprocessing:\n",
        "        ‚Ä¢ Split long documents into smaller chunks\n",
        "        ‚Ä¢ Maintain overlap for context continuity\n",
        "        ‚Ä¢ Optimize for both accuracy and performance\n",
        "        \"\"\"\n",
        "        print(\"‚úÇÔ∏è Starting document chunking process...\")\n",
        "        print(\"   üìè Breaking documents into optimal-sized pieces\")\n",
        "        \n",
        "        # Create the text splitter with intelligent chunking\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.config.CHUNK_SIZE,        # Max characters per chunk\n",
        "            chunk_overlap=self.config.CHUNK_OVERLAP,  # Overlap between chunks\n",
        "            length_function=len,                      # How to measure length\n",
        "        )\n",
        "        \n",
        "        print(f\"   ‚öôÔ∏è Chunking parameters:\")\n",
        "        print(f\"      üìè Chunk size: {self.config.CHUNK_SIZE} characters\") \n",
        "        print(f\"      üîÑ Overlap: {self.config.CHUNK_OVERLAP} characters\")\n",
        "        print(f\"   üîÑ Processing {len(documents)} documents...\")\n",
        "        \n",
        "        # Split the documents\n",
        "        split_documents = text_splitter.split_documents(documents)\n",
        "        \n",
        "        # Show some statistics\n",
        "        avg_chunk_size = sum(len(doc.page_content) for doc in split_documents) / len(split_documents)\n",
        "        \n",
        "        print(f\"‚úÖ Chunking complete!\")\n",
        "        print(f\"   üìä Input: {len(documents)} documents\")\n",
        "        print(f\"   üìÑ Output: {len(split_documents)} chunks\")\n",
        "        print(f\"   üìè Average chunk size: {avg_chunk_size:.0f} characters\")\n",
        "        print(f\"   üéØ Ready for embedding generation!\")\n",
        "        \n",
        "        return split_documents\n",
        "    \n",
        "    def create_vector_store(self, documents):\n",
        "        \"\"\"\n",
        "        üóÑÔ∏è Create Vector Database - The Heart of Semantic Search\n",
        "        \n",
        "        This is the 'Load' phase of our GenAI ETL:\n",
        "        ‚Ä¢ Convert text chunks to embeddings\n",
        "        ‚Ä¢ Store in specialized vector database\n",
        "        ‚Ä¢ Create search interface for similarity queries\n",
        "        \"\"\"\n",
        "        print(\"üóÑÔ∏è Creating vector database...\")\n",
        "        print(\"   üßÆ This will convert all text to vectors and store them\")\n",
        "        \n",
        "        # Initialize embeddings if not already done\n",
        "        if not self.embeddings:\n",
        "            print(\"   üîß Embedding model not initialized, setting it up...\")\n",
        "            self.initialize_embeddings()\n",
        "            \n",
        "        print(\"   üìä Converting documents to vectors...\")\n",
        "        print(\"   ‚è≥ This might take a moment - we're doing math on every piece of text!\")\n",
        "        \n",
        "        # Create ChromaDB vector store\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=self.config.VECTOR_STORE_COLLECTION_NAME,\n",
        "            persist_directory=self.config.VECTOR_STORE_PERSIST_DIRECTORY\n",
        "        )\n",
        "        \n",
        "        print(\"   üîç Setting up similarity search interface...\")\n",
        "        \n",
        "        # Create retriever for similarity search\n",
        "        self.retriever = self.vector_store.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": self.config.SIMILARITY_SEARCH_K}\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Vector store created successfully!\")\n",
        "        print(f\"   üìä Stored {len(documents)} document chunks as vectors\")\n",
        "        print(f\"   üîç Search configured to return top {self.config.SIMILARITY_SEARCH_K} matches\")\n",
        "        print(f\"   üíæ Persisted to: {self.config.VECTOR_STORE_PERSIST_DIRECTORY}\")\n",
        "        print(f\"   üéØ Ready for semantic search!\")\n",
        "        \n",
        "        return self.vector_store\n",
        "    \n",
        "    def initialize_model(self):\n",
        "        \"\"\"\n",
        "        ü§ñ Initialize Gemini 2.5 Pro - The Language Understanding Brain\n",
        "        \n",
        "        This sets up our Large Language Model:\n",
        "        ‚Ä¢ Connects to Google's Gemini 2.5 Pro\n",
        "        ‚Ä¢ Configures response style (temperature)\n",
        "        ‚Ä¢ Prepares for question answering\n",
        "        \"\"\"\n",
        "        print(\"ü§ñ Connecting to Gemini 2.5 Pro...\")\n",
        "        print(\"   üß† This is Google's most advanced language model\")\n",
        "        \n",
        "        self.model = init_chat_model(\n",
        "            model=self.config.MODEL_NAME,\n",
        "            model_provider=self.config.MODEL_PROVIDER,\n",
        "            temperature=self.config.TEMPERATURE\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Gemini 2.5 Pro connected successfully!\")\n",
        "        print(f\"   üå°Ô∏è Temperature: {self.config.TEMPERATURE} (0=factual, 1=creative)\")\n",
        "        print(f\"   üéØ Ready to answer questions!\")\n",
        "        \n",
        "        return self.model\n",
        "    \n",
        "    def create_qa_chain(self):\n",
        "        \"\"\"\n",
        "        üîó Create Question-Answering Pipeline - Putting It All Together\n",
        "        \n",
        "        This combines everything into a complete RAG system:\n",
        "        ‚Ä¢ Retrieval: Find relevant documents\n",
        "        ‚Ä¢ Augmentation: Add context to the question  \n",
        "        ‚Ä¢ Generation: Generate answer using LLM\n",
        "        \"\"\"\n",
        "        print(\"üîó Building Question-Answering pipeline...\")\n",
        "        print(\"   üéØ This combines search + AI to answer questions\")\n",
        "        \n",
        "        # Initialize model if needed\n",
        "        if not self.model:\n",
        "            print(\"   ü§ñ LLM not initialized, setting it up...\")\n",
        "            self.initialize_model()\n",
        "            \n",
        "        if not self.retriever:\n",
        "            raise ValueError(\"‚ùå Vector store must be created first!\")\n",
        "        \n",
        "        print(\"   üìù Creating prompt template...\")\n",
        "        \n",
        "        # Design the prompt template (how we ask the AI)\n",
        "        prompt_template = \"\"\"You are a helpful AI assistant. Use the following pieces of context to answer the question. \n",
        "        \n",
        "If you don't know the answer based on the provided context, say that you don't know. Don't make up information.\n",
        "\n",
        "Context Information:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "        \n",
        "        prompt = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "        \n",
        "        print(\"   üîó Connecting all components...\")\n",
        "        \n",
        "        # Create the complete RAG chain\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.model,                           # Language model\n",
        "            chain_type=\"stuff\",                       # How to combine docs\n",
        "            retriever=self.retriever,                 # Document retriever\n",
        "            chain_type_kwargs={\"prompt\": prompt},     # Custom prompt\n",
        "            return_source_documents=True              # Include source info\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Question-Answering pipeline ready!\")\n",
        "        print(\"   üîç Can now search your documents\")\n",
        "        print(\"   ü§ñ Can answer questions using AI\") \n",
        "        print(\"   üìö Will show source documents\")\n",
        "        print(\"   üéâ Your GenAI system is complete!\")\n",
        "        \n",
        "        return self.qa_chain\n",
        "    \n",
        "    def search_documents(self, query: str, k: int = 4):\n",
        "        \"\"\"\n",
        "        üîç Semantic Document Search - Find Similar Content\n",
        "        \n",
        "        This performs similarity search without AI generation:\n",
        "        ‚Ä¢ Convert query to vector\n",
        "        ‚Ä¢ Find most similar document chunks\n",
        "        ‚Ä¢ Return ranked results\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"‚ùå Vector store must be created first!\")\n",
        "            \n",
        "        k = k if k > 0 else self.config.SIMILARITY_SEARCH_K\n",
        "        print(f\"üîç Searching for documents similar to: '{query}'\")\n",
        "        print(f\"   üìä Looking for top {k} matches...\")\n",
        "        \n",
        "        results = self.vector_store.similarity_search(query, k=k)\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(results)} similar documents\")\n",
        "        return results\n",
        "    \n",
        "    def ask_question(self, question: str):\n",
        "        \"\"\"\n",
        "        ü§î Ask Questions About Your Documents - The Complete RAG Experience\n",
        "        \n",
        "        This is the full RAG (Retrieval-Augmented Generation) process:\n",
        "        1. Find relevant documents (Retrieval)\n",
        "        2. Add them as context (Augmentation)  \n",
        "        3. Generate answer with AI (Generation)\n",
        "        \"\"\"\n",
        "        if not self.qa_chain:\n",
        "            raise ValueError(\"‚ùå QA chain must be created first!\")\n",
        "        \n",
        "        print(f\"ü§î Processing question: '{question}'\")\n",
        "        print(\"üîç Step 1: Searching for relevant information...\")\n",
        "        print(\"ü§ñ Step 2: Generating AI response...\")\n",
        "        print(\"üìö Step 3: Compiling sources...\")\n",
        "        \n",
        "        # Run the complete RAG pipeline\n",
        "        result = self.qa_chain({\"query\": question})\n",
        "        \n",
        "        answer = result[\"result\"]\n",
        "        source_docs = result[\"source_documents\"]\n",
        "        \n",
        "        print(f\"\\n‚úÖ Question answering complete!\")\n",
        "        print(f\"üí° Answer generated using {len(source_docs)} source documents\")\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"source_documents\": source_docs\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Semantic Search Engine class complete!\")\n",
        "print(\"üéâ You've just built a complete RAG (Retrieval-Augmented Generation) system!\")\n",
        "print(\"üí° This combines the best of search engines and AI language models\")\n",
        "print(\"üéØ Next: Let's put it all together and test it!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 5: Putting It All Together - Building Your First GenAI Application!\n",
        "\n",
        "This is the exciting part! We'll now assemble all our components into a working GenAI system.\n",
        "\n",
        "### üéØ **What We're About to Do:**\n",
        "1. **Load documents** from your GCS bucket (Extract)\n",
        "2. **Process and chunk** the text (Transform)  \n",
        "3. **Create vector embeddings** (Feature Engineering)\n",
        "4. **Build the vector database** (Load)\n",
        "5. **Initialize the AI model** (Deploy)\n",
        "6. **Create the QA pipeline** (Production Ready!)\n",
        "\n",
        "### üí° **Learning Focus:**\n",
        "- See how all GenAI components work together\n",
        "- Understand the complete data flow\n",
        "- Watch the RAG pipeline in action\n",
        "- Learn about performance considerations\n",
        "\n",
        "### üîó **The GenAI Pipeline Flow:**\n",
        "```\n",
        "Documents (GCS) ‚Üí Chunking ‚Üí Embeddings ‚Üí Vector DB ‚Üí Retrieval ‚Üí LLM ‚Üí Answer\n",
        "```\n",
        "\n",
        "Ready to see your GenAI system come to life? Let's go!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéâ Welcome to GenAI System Assembly!\n",
            "Let's build your semantic search engine step by step...\n",
            "üéØ This is like deploying a complete data pipeline, but for AI!\n",
            "\n",
            "üîß INITIALIZATION üîß\n",
            "Creating the main engine instance...\n",
            "üîß Initializing Semantic Search Engine...\n",
            "   üìä Engine initialized with lazy loading pattern\n",
            "   üéØ Components will be created when needed\n",
            "\n",
            "============================================================\n",
            "üìö STEP 1: DOCUMENT INGESTION (Extract Phase)\n",
            "============================================================\n",
            "üéØ Learning Goal: Understand how GenAI systems ingest data\n",
            "üí° This is like your ETL extract phase - getting data from source\n",
            "\n",
            "ü™£ Connecting to GCS bucket: genai-sai-bucket\n",
            "üîß Initializing Document Loader...\n",
            "   üìç Target: gs://genai-sai-bucket/\n",
            "\n",
            "üì• Starting document ingestion...\n",
            "‚è≥ This process will:\n",
            "   1. Connect to your GCS bucket\n",
            "   2. Scan for supported file types\n",
            "   3. Download and parse each document\n",
            "   4. Extract text content\n",
            "   5. Add metadata for tracking\n",
            "üì• Starting document ingestion pipeline...\n",
            "üîë Connecting to Google Cloud Storage...\n",
            "   üìã Using credentials: genai-sai-auth.json\n",
            "‚úÖ Connected to GCS project: igneous-future-451513-v8d\n",
            "üîê Authentication successful!\n",
            "üìã Scanning for documents in GCS bucket...\n",
            "   üéØ Looking for: .pdf, .docx, .txt files\n",
            "   üìÑ Found: Metro AG ‚Äì Wikipedia.pdf\n",
            "‚úÖ Discovery complete: 1 documents found\n",
            "üöÄ Processing 1 documents...\n",
            "\n",
            "üìÑ Processing 1/1: Metro AG ‚Äì Wikipedia.pdf\n",
            "   üìÅ Creating temp file with suffix: .pdf\n",
            "   ‚¨áÔ∏è Downloading from GCS...\n",
            "   ‚úÖ Downloaded to: /var/folders/7x/3vpj1fqx717cs602xgtbss700000gn/T/tmpn8ysxp60.pdf\n",
            "   üîß Selecting parser for: .pdf\n",
            "   üìñ Using PDF parser\n",
            "   üìñ Parsing document content...\n",
            "   üìä Extracted 18 pages/sections\n",
            "   üè∑Ô∏è Adding metadata...\n",
            "   ‚úÖ Successfully processed: Metro AG ‚Äì Wikipedia.pdf\n",
            "   üßπ Cleaned up temp file\n",
            "\n",
            "üéâ Ingestion Complete!\n",
            "   üìä Total documents processed: 1\n",
            "   üìÑ Total text chunks extracted: 18\n",
            "   üéØ Ready for the next pipeline stage!\n",
            "\n",
            "üìä INGESTION RESULTS:\n",
            "   ‚úÖ Documents loaded: 18\n",
            "   üìÑ Sample sources: ['Metro AG ‚Äì Wikipedia.pdf']\n",
            "   üìè Total text characters: 60,710\n",
            "   üíæ Average document size: 3,372 characters\n",
            "\n",
            "üí° What just happened:\n",
            "   ‚Ä¢ Your documents are now in memory as text objects\n",
            "   ‚Ä¢ Each has metadata (source file, bucket info)\n",
            "   ‚Ä¢ Ready for the next stage: chunking!\n"
          ]
        }
      ],
      "source": [
        "# üöÄ BUILDING YOUR GENAI SYSTEM - Step by Step!\n",
        "print(\"üéâ Welcome to GenAI System Assembly!\")\n",
        "print(\"Let's build your semantic search engine step by step...\")\n",
        "print(\"üéØ This is like deploying a complete data pipeline, but for AI!\")\n",
        "\n",
        "print(\"\\n\" + \"üîß\" + \" INITIALIZATION \" + \"üîß\")\n",
        "print(\"Creating the main engine instance...\")\n",
        "search_engine = SemanticSearchEngine(config)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìö STEP 1: DOCUMENT INGESTION (Extract Phase)\")\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ Learning Goal: Understand how GenAI systems ingest data\")\n",
        "print(\"üí° This is like your ETL extract phase - getting data from source\")\n",
        "\n",
        "print(f\"\\nü™£ Connecting to GCS bucket: {config.GCS_BUCKET_NAME}\")\n",
        "document_loader = GCSDocumentLoader(\n",
        "    bucket_name=config.GCS_BUCKET_NAME,\n",
        "    project_id=config.GOOGLE_CLOUD_PROJECT,\n",
        "    folder_path=config.GCS_FOLDER_PATH\n",
        ")\n",
        "\n",
        "print(\"\\nüì• Starting document ingestion...\")\n",
        "print(\"‚è≥ This process will:\")\n",
        "print(\"   1. Connect to your GCS bucket\")\n",
        "print(\"   2. Scan for supported file types\")\n",
        "print(\"   3. Download and parse each document\")\n",
        "print(\"   4. Extract text content\")\n",
        "print(\"   5. Add metadata for tracking\")\n",
        "\n",
        "documents = document_loader.download_and_load_documents()\n",
        "\n",
        "print(f\"\\nüìä INGESTION RESULTS:\")\n",
        "print(f\"   ‚úÖ Documents loaded: {len(documents)}\")\n",
        "if documents:\n",
        "    sources = list(set([doc.metadata.get('source', 'Unknown') for doc in documents[:3]]))\n",
        "    print(f\"   üìÑ Sample sources: {sources}\")\n",
        "    total_chars = sum(len(doc.page_content) for doc in documents)\n",
        "    print(f\"   üìè Total text characters: {total_chars:,}\")\n",
        "    print(f\"   üíæ Average document size: {total_chars//len(documents):,} characters\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è No documents found - check your bucket configuration!\")\n",
        "    \n",
        "print(f\"\\nüí° What just happened:\")\n",
        "print(f\"   ‚Ä¢ Your documents are now in memory as text objects\")\n",
        "print(f\"   ‚Ä¢ Each has metadata (source file, bucket info)\")\n",
        "print(f\"   ‚Ä¢ Ready for the next stage: chunking!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "‚úÇÔ∏è STEP 2: DOCUMENT PROCESSING (Transform Phase)\n",
            "============================================================\n",
            "üéØ Learning Goal: Understand text chunking and why it matters\n",
            "üí° This is like data preprocessing - preparing data for optimal AI processing\n",
            "\n",
            "üìè About to process 18 documents\n",
            "üîç Why do we need chunking?\n",
            "   ‚Ä¢ AI models have context limits (like memory constraints)\n",
            "   ‚Ä¢ Smaller chunks = more precise retrieval\n",
            "   ‚Ä¢ Overlap ensures we don't lose context at boundaries\n",
            "\n",
            "‚öôÔ∏è Chunking configuration:\n",
            "   üìè Chunk size: 1000 characters\n",
            "   üîÑ Overlap: 200 characters\n",
            "   üí° This balances precision vs context!\n",
            "‚úÇÔ∏è Starting document chunking process...\n",
            "   üìè Breaking documents into optimal-sized pieces\n",
            "   ‚öôÔ∏è Chunking parameters:\n",
            "      üìè Chunk size: 1000 characters\n",
            "      üîÑ Overlap: 200 characters\n",
            "   üîÑ Processing 18 documents...\n",
            "‚úÖ Chunking complete!\n",
            "   üìä Input: 18 documents\n",
            "   üìÑ Output: 81 chunks\n",
            "   üìè Average chunk size: 879 characters\n",
            "   üéØ Ready for embedding generation!\n",
            "\n",
            "============================================================\n",
            "üßÆ STEP 3: VECTOR EMBEDDING CREATION (Feature Engineering)\n",
            "============================================================\n",
            "üéØ Learning Goal: Understand how text becomes searchable vectors\n",
            "üí° This converts human language to mathematical representations\n",
            "\n",
            "üîÆ The embedding magic:\n",
            "   üìù 'Machine learning' ‚Üí [0.2, -0.1, 0.8, ...384 numbers]\n",
            "   üìù 'AI algorithms' ‚Üí [0.3, -0.2, 0.7, ...384 numbers]\n",
            "   üéØ Similar concepts get similar numbers!\n",
            "\n",
            "üóÑÔ∏è Creating vector database with 81 chunks...\n",
            "‚è≥ This will take a moment - we're doing math on every piece of text!\n",
            "üóÑÔ∏è Creating vector database...\n",
            "   üßÆ This will convert all text to vectors and store them\n",
            "   üîß Embedding model not initialized, setting it up...\n",
            "üßÆ Setting up embedding model...\n",
            "   üìê This converts text to vectors (numbers that capture meaning)\n",
            "‚úÖ Embedding model ready: sentence-transformers/all-MiniLM-L6-v2\n",
            "   üí° Now we can convert any text to vectors!\n",
            "   üìä Example: 'Hello' ‚Üí [0.1, 0.8, -0.3, ...384 numbers]\n",
            "   üìä Converting documents to vectors...\n",
            "   ‚è≥ This might take a moment - we're doing math on every piece of text!\n",
            "   üîç Setting up similarity search interface...\n",
            "‚úÖ Vector store created successfully!\n",
            "   üìä Stored 81 document chunks as vectors\n",
            "   üîç Search configured to return top 4 matches\n",
            "   üíæ Persisted to: ./vector_store\n",
            "   üéØ Ready for semantic search!\n",
            "\n",
            "üìä PROCESSING COMPLETE!\n",
            "   üìö Original documents: 18\n",
            "   ‚úÇÔ∏è Text chunks created: 81\n",
            "   üßÆ Vector embeddings: 81 (one per chunk)\n",
            "   üóÑÔ∏è Vector database: ‚úÖ Ready for semantic search!\n",
            "\n",
            "üí° What you now have:\n",
            "   ‚Ä¢ Every piece of text is now a vector in all-MiniLM-L6-v2\n",
            "   ‚Ä¢ You can find 'similar' content mathematically\n",
            "   ‚Ä¢ The AI can search by meaning, not just keywords!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÇÔ∏è STEP 2: DOCUMENT PROCESSING (Transform Phase)\")\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ Learning Goal: Understand text chunking and why it matters\")\n",
        "print(\"üí° This is like data preprocessing - preparing data for optimal AI processing\")\n",
        "\n",
        "if documents:\n",
        "    print(f\"\\nüìè About to process {len(documents)} documents\")\n",
        "    print(\"üîç Why do we need chunking?\")\n",
        "    print(\"   ‚Ä¢ AI models have context limits (like memory constraints)\")\n",
        "    print(\"   ‚Ä¢ Smaller chunks = more precise retrieval\")\n",
        "    print(\"   ‚Ä¢ Overlap ensures we don't lose context at boundaries\")\n",
        "    \n",
        "    print(f\"\\n‚öôÔ∏è Chunking configuration:\")\n",
        "    print(f\"   üìè Chunk size: {config.CHUNK_SIZE} characters\")\n",
        "    print(f\"   üîÑ Overlap: {config.CHUNK_OVERLAP} characters\")\n",
        "    print(f\"   üí° This balances precision vs context!\")\n",
        "    \n",
        "    # Process documents (split into chunks)\n",
        "    processed_docs = search_engine.process_documents(documents)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üßÆ STEP 3: VECTOR EMBEDDING CREATION (Feature Engineering)\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üéØ Learning Goal: Understand how text becomes searchable vectors\")\n",
        "    print(\"üí° This converts human language to mathematical representations\")\n",
        "    \n",
        "    print(\"\\nüîÆ The embedding magic:\")\n",
        "    print(\"   üìù 'Machine learning' ‚Üí [0.2, -0.1, 0.8, ...384 numbers]\")\n",
        "    print(\"   üìù 'AI algorithms' ‚Üí [0.3, -0.2, 0.7, ...384 numbers]\")\n",
        "    print(\"   üéØ Similar concepts get similar numbers!\")\n",
        "    \n",
        "    print(f\"\\nüóÑÔ∏è Creating vector database with {len(processed_docs)} chunks...\")\n",
        "    print(\"‚è≥ This will take a moment - we're doing math on every piece of text!\")\n",
        "    \n",
        "    # Create vector store\n",
        "    vector_store = search_engine.create_vector_store(processed_docs)\n",
        "    \n",
        "    print(f\"\\nüìä PROCESSING COMPLETE!\")\n",
        "    print(f\"   üìö Original documents: {len(documents)}\")\n",
        "    print(f\"   ‚úÇÔ∏è Text chunks created: {len(processed_docs)}\")\n",
        "    print(f\"   üßÆ Vector embeddings: {len(processed_docs)} (one per chunk)\")\n",
        "    print(f\"   üóÑÔ∏è Vector database: ‚úÖ Ready for semantic search!\")\n",
        "    \n",
        "    print(f\"\\nüí° What you now have:\")\n",
        "    print(f\"   ‚Ä¢ Every piece of text is now a vector in {config.EMBEDDING_MODEL.split('/')[-1]}\")\n",
        "    print(f\"   ‚Ä¢ You can find 'similar' content mathematically\")\n",
        "    print(f\"   ‚Ä¢ The AI can search by meaning, not just keywords!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No documents found. Please check your GCS bucket and configuration.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ü§ñ STEP 4: AI MODEL INTEGRATION (The Brain)\n",
            "============================================================\n",
            "üéØ Learning Goal: Understand how LLMs integrate with search\n",
            "üí° This connects Google's most advanced AI to your data\n",
            "\n",
            "üß† Connecting to gemini-2.5-pro...\n",
            "üåü What makes Gemini 2.5 Pro special:\n",
            "   ‚Ä¢ Understands context and nuance\n",
            "   ‚Ä¢ Can reason across multiple documents\n",
            "   ‚Ä¢ Generates human-like responses\n",
            "   ‚Ä¢ Knows when it doesn't know something\n",
            "\n",
            "üîó Building the complete RAG pipeline...\n",
            "üìã RAG = Retrieval-Augmented Generation\n",
            "   1. üîç Retrieve: Find relevant documents\n",
            "   2. üìù Augment: Add context to the question\n",
            "   3. ü§ñ Generate: Create AI-powered answer\n",
            "üîó Building Question-Answering pipeline...\n",
            "   üéØ This combines search + AI to answer questions\n",
            "   ü§ñ LLM not initialized, setting it up...\n",
            "ü§ñ Connecting to Gemini 2.5 Pro...\n",
            "   üß† This is Google's most advanced language model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Gemini 2.5 Pro connected successfully!\n",
            "   üå°Ô∏è Temperature: 0.0 (0=factual, 1=creative)\n",
            "   üéØ Ready to answer questions!\n",
            "   üìù Creating prompt template...\n",
            "   üîó Connecting all components...\n",
            "‚úÖ Question-Answering pipeline ready!\n",
            "   üîç Can now search your documents\n",
            "   ü§ñ Can answer questions using AI\n",
            "   üìö Will show source documents\n",
            "   üéâ Your GenAI system is complete!\n",
            "\n",
            "üéâ YOUR GENAI SYSTEM IS READY!\n",
            "========================================\n",
            "   ü§ñ AI Model: gemini-2.5-pro ‚úÖ\n",
            "   üìö Document Store: 81 chunks ‚úÖ\n",
            "   üßÆ Vector Database: ChromaDB ‚úÖ\n",
            "   üîó QA Pipeline: Complete RAG system ‚úÖ\n",
            "   üéØ Ready to answer questions! üöÄ\n",
            "\n",
            "üí° What you've built:\n",
            "   ‚Ä¢ A complete GenAI application from scratch\n",
            "   ‚Ä¢ Enterprise-grade document search system\n",
            "   ‚Ä¢ AI that can understand and answer questions about YOUR data\n",
            "   ‚Ä¢ Production-ready RAG (Retrieval-Augmented Generation) pipeline\n",
            "\n",
            "üöÄ Next: Let's test your creation!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ü§ñ STEP 4: AI MODEL INTEGRATION (The Brain)\")\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ Learning Goal: Understand how LLMs integrate with search\")\n",
        "print(\"üí° This connects Google's most advanced AI to your data\")\n",
        "\n",
        "if search_engine.vector_store:\n",
        "    print(f\"\\nüß† Connecting to {config.MODEL_NAME}...\")\n",
        "    print(\"üåü What makes Gemini 2.5 Pro special:\")\n",
        "    print(\"   ‚Ä¢ Understands context and nuance\")\n",
        "    print(\"   ‚Ä¢ Can reason across multiple documents\")\n",
        "    print(\"   ‚Ä¢ Generates human-like responses\")\n",
        "    print(\"   ‚Ä¢ Knows when it doesn't know something\")\n",
        "    \n",
        "    print(f\"\\nüîó Building the complete RAG pipeline...\")\n",
        "    print(\"üìã RAG = Retrieval-Augmented Generation\")\n",
        "    print(\"   1. üîç Retrieve: Find relevant documents\")\n",
        "    print(\"   2. üìù Augment: Add context to the question\")\n",
        "    print(\"   3. ü§ñ Generate: Create AI-powered answer\")\n",
        "    \n",
        "    # Initialize the model and create QA chain\n",
        "    qa_chain = search_engine.create_qa_chain()\n",
        "    \n",
        "    print(f\"\\nüéâ YOUR GENAI SYSTEM IS READY!\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"   ü§ñ AI Model: {config.MODEL_NAME} ‚úÖ\")\n",
        "    print(f\"   üìö Document Store: {len(processed_docs)} chunks ‚úÖ\") \n",
        "    print(f\"   üßÆ Vector Database: ChromaDB ‚úÖ\")\n",
        "    print(f\"   üîó QA Pipeline: Complete RAG system ‚úÖ\")\n",
        "    print(f\"   üéØ Ready to answer questions! üöÄ\")\n",
        "    \n",
        "    print(f\"\\nüí° What you've built:\")\n",
        "    print(f\"   ‚Ä¢ A complete GenAI application from scratch\")\n",
        "    print(f\"   ‚Ä¢ Enterprise-grade document search system\")\n",
        "    print(f\"   ‚Ä¢ AI that can understand and answer questions about YOUR data\")\n",
        "    print(f\"   ‚Ä¢ Production-ready RAG (Retrieval-Augmented Generation) pipeline\")\n",
        "    \n",
        "    print(f\"\\nüöÄ Next: Let's test your creation!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Vector store not available. Cannot create QA chain.\")\n",
        "    print(\"üí° Make sure Step 2 completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 6: Testing Your GenAI System - See the Magic in Action!\n",
        "\n",
        "Time to test your creation! This is where you'll see the power of GenAI firsthand.\n",
        "\n",
        "### üéØ **What You'll Learn by Testing:**\n",
        "- How semantic search finds relevant content\n",
        "- How AI generates answers from your documents  \n",
        "- The importance of source attribution\n",
        "- How to evaluate GenAI system performance\n",
        "\n",
        "### üîç **Types of Questions to Try:**\n",
        "1. **Factual**: \"What is...?\" \"When did...?\" \"How many...?\"\n",
        "2. **Analytical**: \"Why does...?\" \"What are the implications...?\"\n",
        "3. **Comparative**: \"How does X compare to Y?\" \n",
        "4. **Summary**: \"Summarize the main points about...\"\n",
        "5. **Exploratory**: \"What does the document say about...?\"\n",
        "\n",
        "### üí° **Understanding the Results:**\n",
        "- **Answer**: AI-generated response based on your documents\n",
        "- **Sources**: Which document chunks were used (like data lineage!)\n",
        "- **Confidence**: How well the retrieved documents match your question\n",
        "\n",
        "Let's start with an automated test to see your system in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Semantic Search Engine\n",
            "==================================================\n",
            "\n",
            "üìù Sample Question: What is the main topic discussed in the documents?\n",
            "--------------------------------------------------\n",
            "ü§î Processing question: 'What is the main topic discussed in the documents?'\n",
            "üîç Step 1: Searching for relevant information...\n",
            "ü§ñ Step 2: Generating AI response...\n",
            "üìö Step 3: Compiling sources...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Question answering complete!\n",
            "üí° Answer generated using 4 source documents\n",
            "\n",
            "üí° Answer:\n",
            "Based on the provided context, the main topic discussed in the documents is the company **Metro AG**.\n",
            "\n",
            "The text is a list of sources, the majority of which are annual reports (\"Gesch√§ftsberichte\") from Metro AG for various years. Other sources mentioned relate to Metro AG's share ownership, corporate structure, and legal cases.\n",
            "\n",
            "üìö Source Documents:\n",
            "   1. Metro AG ‚Äì Wikipedia.pdf (Preview: Gesch√§ftsf√ºhrung, 11.¬†April 2025, abgerufen am 17.¬†April 2025.\n",
            "100. Reuters: Investor Kretinsky kont...)\n",
            "   2. Metro AG ‚Äì Wikipedia.pdf (Preview: 103. Gesch√§ftsbericht 2018/19. (https://berichte.metroag.de/geschaeftsbericht/2018-2019/servicesei\n",
            "t...)\n",
            "   3. Metro AG ‚Äì Wikipedia.pdf (Preview: 36. Die Gesch√§ftsbereiche der Metro-Gruppe (https://web.archive.org/web/20080330120141/http://\n",
            "www.n...)\n",
            "   4. Metro AG ‚Äì Wikipedia.pdf (Preview: 107. Gesch√§ftsbericht 2022/23. (https://berichte.metroag.de/geschaeftsbericht/2022-2023/_assets/d\n",
            "ow...)\n",
            "\n",
            "üí° Try other questions:\n",
            "   2. Can you summarize the key points?\n",
            "   3. What are the main conclusions or findings?\n",
            "   4. Are there any specific recommendations mentioned?\n",
            "   5. What methodology or approach is described?\n"
          ]
        }
      ],
      "source": [
        "# Test the semantic search engine with sample questions\n",
        "def test_search_engine():\n",
        "    \"\"\"Test function to demonstrate the search engine capabilities.\"\"\"\n",
        "    \n",
        "    if not search_engine.qa_chain:\n",
        "        print(\"‚ùå QA chain not available. Please run the setup cells first.\")\n",
        "        return\n",
        "    \n",
        "    # Sample questions - modify these based on your documents\n",
        "    sample_questions = [\n",
        "        \"What is the main topic discussed in the documents?\",\n",
        "        \"Can you summarize the key points?\",\n",
        "        \"What are the main conclusions or findings?\",\n",
        "        \"Are there any specific recommendations mentioned?\",\n",
        "        \"What methodology or approach is described?\"\n",
        "    ]\n",
        "    \n",
        "    print(\"üß™ Testing Semantic Search Engine\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Test with the first sample question\n",
        "    question = sample_questions[0]\n",
        "    print(f\"\\nüìù Sample Question: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        result = search_engine.ask_question(question)\n",
        "        \n",
        "        print(f\"\\nüí° Answer:\")\n",
        "        print(f\"{result['answer']}\")\n",
        "        \n",
        "        print(f\"\\nüìö Source Documents:\")\n",
        "        for i, doc in enumerate(result['source_documents'], 1):\n",
        "            print(f\"   {i}. {doc.metadata.get('source', 'Unknown')} (Preview: {doc.page_content[:100]}...)\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during question answering: {str(e)}\")\n",
        "    \n",
        "    print(f\"\\nüí° Try other questions:\")\n",
        "    for i, q in enumerate(sample_questions[1:], 2):\n",
        "        print(f\"   {i}. {q}\")\n",
        "\n",
        "# Run the test\n",
        "test_search_engine()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interactive Question Answering\n",
        "\n",
        "Use this cell to ask your own questions about the documents:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Interactive question function ready!\n",
            "Usage: ask_custom_question('Your question about the documents')\n"
          ]
        }
      ],
      "source": [
        "# Interactive question answering\n",
        "def ask_custom_question(question: str):\n",
        "    \"\"\"Ask a custom question to your documents.\"\"\"\n",
        "    \n",
        "    if not search_engine.qa_chain:\n",
        "        print(\"‚ùå QA chain not available. Please run the setup cells first.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"ü§î Your Question: {question}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    try:\n",
        "        result = search_engine.ask_question(question)\n",
        "        \n",
        "        print(f\"\\nüí° Answer:\")\n",
        "        print(f\"{result['answer']}\")\n",
        "        \n",
        "        print(f\"\\nüìö Sources:\")\n",
        "        for i, doc in enumerate(result['source_documents'], 1):\n",
        "            source = doc.metadata.get('source', 'Unknown')\n",
        "            preview = doc.page_content[:150].replace('\\n', ' ')\n",
        "            print(f\"   {i}. {source}\")\n",
        "            print(f\"      Preview: {preview}...\")\n",
        "            \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# result = ask_custom_question(\"Your question here\")\n",
        "\n",
        "print(\"‚úÖ Interactive question function ready!\")\n",
        "print(\"Usage: ask_custom_question('Your question about the documents')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Search Similar Documents\n",
        "\n",
        "You can also search for documents similar to a query without asking questions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Document search function ready!\n",
            "Usage: search_similar_documents('your search query', k=3)\n"
          ]
        }
      ],
      "source": [
        "# Search for similar documents\n",
        "def search_similar_documents(query: str, k: int = 3):\n",
        "    \"\"\"Search for documents similar to a query.\"\"\"\n",
        "    \n",
        "    if not search_engine.vector_store:\n",
        "        print(\"‚ùå Vector store not available. Please run the setup cells first.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üîç Searching for: '{query}'\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    try:\n",
        "        results = search_engine.search_documents(query, k=k)\n",
        "        \n",
        "        print(f\"\\nüìÑ Found {len(results)} similar documents:\")\n",
        "        \n",
        "        for i, doc in enumerate(results, 1):\n",
        "            source = doc.metadata.get('source', 'Unknown')\n",
        "            content_preview = doc.page_content[:200].replace('\\n', ' ')\n",
        "            \n",
        "            print(f\"\\n{i}. Source: {source}\")\n",
        "            print(f\"   Content Preview: {content_preview}...\")\n",
        "            print(f\"   Length: {len(doc.page_content)} characters\")\n",
        "            \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during search: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# results = search_similar_documents(\"your search query\", k=3)\n",
        "\n",
        "print(\"‚úÖ Document search function ready!\")\n",
        "print(\"Usage: search_similar_documents('your search query', k=3)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully built a semantic search engine with the following features:\n",
        "\n",
        "### ‚úÖ What You've Built:\n",
        "- **Document Loading**: Automatically loads PDF, DOCX, and TXT files from GCS bucket\n",
        "- **Text Processing**: Intelligently splits documents into searchable chunks\n",
        "- **Vector Search**: Uses advanced embeddings for semantic similarity search\n",
        "- **AI Question Answering**: Powered by Gemini 2.5 Pro for natural language responses\n",
        "- **Source Attribution**: Shows which documents were used to answer questions\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "1. **Add More Documents**: Upload more files to your GCS bucket and re-run the setup\n",
        "2. **Customize Parameters**: Adjust chunk size, embedding models, or search parameters in the config\n",
        "3. **Advanced Features**: Add document filtering, different prompt templates, or conversation memory\n",
        "4. **UI Integration**: Build a web interface using Streamlit or Gradio\n",
        "5. **Production Setup**: Add error handling, logging, and monitoring\n",
        "\n",
        "### üõ†Ô∏è Configuration Summary:\n",
        "- **GCS Bucket**: `{config.GCS_BUCKET_NAME}`\n",
        "- **Model**: `{config.MODEL_NAME}`\n",
        "- **Embedding Model**: `{config.EMBEDDING_MODEL}`\n",
        "- **Chunk Size**: `{config.CHUNK_SIZE} characters`\n",
        "\n",
        "### üìù Usage Examples:\n",
        "```python\n",
        "# Ask questions about your documents\n",
        "result = ask_custom_question(\"What are the main findings?\")\n",
        "\n",
        "# Search for similar content\n",
        "docs = search_similar_documents(\"machine learning applications\")\n",
        "```\n",
        "\n",
        "Happy searching! üîç‚ú®\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
