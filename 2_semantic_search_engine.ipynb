{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“ Learning GenAI: Building a Semantic Search Engine\n",
        "## *From GCP Data Engineer to GenAI Developer*\n",
        "\n",
        "Welcome to your GenAI learning journey! This notebook will teach you how to build a **Semantic Search Engine** step by step.\n",
        "\n",
        "### ğŸ§  **What You'll Learn:**\n",
        "1. **RAG (Retrieval-Augmented Generation)** - How to make AI smarter with your own data\n",
        "2. **Vector Embeddings** - Converting text into mathematical representations\n",
        "3. **Semantic Search** - Finding meaning, not just keywords\n",
        "4. **LangChain Framework** - The toolkit for GenAI applications\n",
        "5. **Document Processing** - From raw files to AI-ready chunks\n",
        "\n",
        "### ğŸ—ï¸ **What We're Building:**\n",
        "A system that can:\n",
        "- ğŸ“š Read your documents from GCS (like your data pipelines!)\n",
        "- ğŸ§® Convert text into vectors (embeddings)\n",
        "- ğŸ” Find relevant information semantically \n",
        "- ğŸ¤– Answer questions using Gemini 2.5 Pro\n",
        "- ğŸ“ Show sources (like good data lineage!)\n",
        "\n",
        "### ğŸ’¡ **Key Concepts We'll Explore:**\n",
        "- **Embeddings**: Text â†’ Numbers that capture meaning\n",
        "- **Vector Database**: Storage optimized for similarity search\n",
        "- **Chunking**: Breaking documents into digestible pieces\n",
        "- **Retrieval**: Finding relevant context for questions\n",
        "- **Generation**: Using LLM to create answers from context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ Step 1: Understanding & Installing Dependencies\n",
        "\n",
        "Before we build our GenAI application, let's understand what each tool does:\n",
        "\n",
        "### ğŸ”§ **Core GenAI Stack:**\n",
        "- **`langchain`**: The main framework - think of it as pandas for GenAI\n",
        "- **`langchain-google-vertexai`**: Connects LangChain to Google's AI models\n",
        "- **`sentence-transformers`**: Creates embeddings (text â†’ vectors)\n",
        "- **`chromadb`**: Vector database for similarity search\n",
        "- **`PyPDF2`, `python-docx`**: Document parsers (like reading CSV/JSON files)\n",
        "\n",
        "### ğŸ¤” **Why These Specific Packages?**\n",
        "- **LangChain**: Simplifies complex GenAI workflows (like Apache Beam for data)\n",
        "- **Embeddings**: Convert text to numbers so computers can understand similarity\n",
        "- **Vector DB**: Specialized storage for finding \"similar\" items quickly\n",
        "- **Document Loaders**: Handle different file formats automatically\n",
        "\n",
        "Let's install everything with compatible versions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Installing packages for our GenAI application...\n",
            "This is like setting up your data engineering environment, but for AI!\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "âœ… Installation complete!\n",
            "ğŸ‰ You now have a complete GenAI development environment!\n",
            "\n",
            "ğŸ“š What we just installed:\n",
            "   â€¢ LangChain: GenAI application framework\n",
            "   â€¢ ChromaDB: Vector database for semantic search\n",
            "   â€¢ Sentence Transformers: Text embedding models\n",
            "   â€¢ Document parsers: PDF, DOCX, TXT support\n",
            "   â€¢ GCS integration: Connect to your cloud storage\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ Installing the GenAI Toolkit\n",
        "print(\"ğŸ”§ Installing packages for our GenAI application...\")\n",
        "print(\"This is like setting up your data engineering environment, but for AI!\")\n",
        "\n",
        "# Core LangChain framework and Google integrations\n",
        "%pip install -qU langchain langchain-google-vertexai langchain-community langchain-text-splitters\n",
        "\n",
        "# Vector storage and embeddings\n",
        "%pip install -qU \"google-cloud-storage<3.0.0,>=2.18.0\" chromadb sentence-transformers\n",
        "\n",
        "# Document processing utilities\n",
        "%pip install -qU python-dotenv PyPDF2 python-docx unstructured tiktoken faiss-cpu\n",
        "\n",
        "print(\"\\nâœ… Installation complete!\")\n",
        "print(\"ğŸ‰ You now have a complete GenAI development environment!\")\n",
        "print(\"\\nğŸ“š What we just installed:\")\n",
        "print(\"   â€¢ LangChain: GenAI application framework\")\n",
        "print(\"   â€¢ ChromaDB: Vector database for semantic search\") \n",
        "print(\"   â€¢ Sentence Transformers: Text embedding models\")\n",
        "print(\"   â€¢ Document parsers: PDF, DOCX, TXT support\")\n",
        "print(\"   â€¢ GCS integration: Connect to your cloud storage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“š Importing libraries - let me explain what each does...\n",
            "âœ… Standard Python utilities imported\n",
            "\n",
            "ğŸ¦œ Importing LangChain components:\n",
            "   â€¢ init_chat_model: Connects to Gemini 2.5 Pro\n",
            "   â€¢ Document loaders: Read PDF, DOCX, TXT files\n",
            "   â€¢ Text splitter: Breaks documents into chunks\n",
            "   â€¢ Embeddings: Convert text to vectors\n",
            "   â€¢ ChromaDB: Vector database for similarity search\n",
            "   â€¢ RetrievalQA: Combines search + question answering\n",
            "   â€¢ PromptTemplate: Structure how we ask the AI\n",
            "\n",
            "â˜ï¸ Google Cloud components:\n",
            "   â€¢ GCS Storage: Access your cloud documents\n",
            "\n",
            "ğŸ‰ All imports successful!\n",
            "ğŸ’¡ Think of LangChain as your 'pandas for GenAI' - it handles the complex stuff!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“š Importing Our GenAI Toolkit\n",
        "print(\"ğŸ“š Importing libraries - let me explain what each does...\")\n",
        "\n",
        "# Standard Python libraries (you know these!)\n",
        "import os\n",
        "import tempfile\n",
        "from typing import List, Dict, Any\n",
        "from pathlib import Path\n",
        "print(\"âœ… Standard Python utilities imported\")\n",
        "\n",
        "# ğŸ¦œ LangChain Core Components\n",
        "print(\"\\nğŸ¦œ Importing LangChain components:\")\n",
        "from langchain.chat_models import init_chat_model  # Initialize AI models\n",
        "print(\"   â€¢ init_chat_model: Connects to Gemini 2.5 Pro\")\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "print(\"   â€¢ Document loaders: Read PDF, DOCX, TXT files\")\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "print(\"   â€¢ Text splitter: Breaks documents into chunks\")\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "print(\"   â€¢ Embeddings: Convert text to vectors\")\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "print(\"   â€¢ ChromaDB: Vector database for similarity search\")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "print(\"   â€¢ RetrievalQA: Combines search + question answering\")\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "print(\"   â€¢ PromptTemplate: Structure how we ask the AI\")\n",
        "\n",
        "# â˜ï¸ Google Cloud Integration\n",
        "print(\"\\nâ˜ï¸ Google Cloud components:\")\n",
        "from google.cloud import storage\n",
        "print(\"   â€¢ GCS Storage: Access your cloud documents\")\n",
        "\n",
        "# Clean up warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\nğŸ‰ All imports successful!\")\n",
        "print(\"ğŸ’¡ Think of LangChain as your 'pandas for GenAI' - it handles the complex stuff!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Step 2: Configuration - Setting Up Your GenAI Pipeline\n",
        "\n",
        "Just like configuring a data pipeline, we need to set parameters for our GenAI application.\n",
        "\n",
        "### ğŸ¯ **Configuration Concepts:**\n",
        "- **Chunk Size**: How big should text pieces be? (like batch size in data processing)\n",
        "- **Overlap**: How much text to share between chunks (ensures continuity)\n",
        "- **Model Temperature**: How creative should AI responses be? (0=precise, 1=creative)\n",
        "- **Similarity Search K**: How many relevant documents to retrieve?\n",
        "\n",
        "### ğŸ”§ **Think of This As:**\n",
        "- Spark configuration for distributed processing\n",
        "- Database connection settings  \n",
        "- ETL pipeline parameters\n",
        "\n",
        "Let's configure our system:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš™ï¸ Setting up configuration - like your data pipeline configs!\n",
            "ğŸ—ï¸ Infrastructure configuration...\n",
            "ğŸ¤– AI model configuration...\n",
            "ğŸ“„ Document processing configuration...\n",
            "ğŸ—„ï¸ Vector storage configuration...\n",
            "ğŸ§® Embedding model configuration...\n",
            "ğŸ” Search configuration...\n",
            "\n",
            "âœ… Configuration Complete!\n",
            "ğŸ“Š Configuration Summary:\n",
            "   ğŸª£ Data Source: genai-sai-bucket\n",
            "   ğŸ¤– AI Model: gemini-2.5-pro\n",
            "   ğŸ§® Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   ğŸ“ Chunk Size: 1000 characters\n",
            "   ğŸ” Retrieval Count: 4 documents\n",
            "\n",
            "ğŸ’¡ Pro Tip: These settings affect performance and accuracy!\n",
            "   â€¢ Larger chunks = more context but slower processing\n",
            "   â€¢ Higher K = more context but potential noise\n",
            "   â€¢ Temperature 0 = factual, higher = creative\n"
          ]
        }
      ],
      "source": [
        "# âš™ï¸ GenAI Configuration Class\n",
        "print(\"âš™ï¸ Setting up configuration - like your data pipeline configs!\")\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration for our GenAI Semantic Search Engine\"\"\"\n",
        "    \n",
        "    # ğŸ—ï¸ Infrastructure Settings (like your GCP project setup)\n",
        "    print(\"ğŸ—ï¸ Infrastructure configuration...\")\n",
        "    GCS_BUCKET_NAME = \"genai-sai-bucket\"  # ğŸª£ Your data storage\n",
        "    GOOGLE_CLOUD_PROJECT = \"igneous-future-451513-v8d\"  # ğŸŒ Your GCP project\n",
        "    GCS_FOLDER_PATH = \"\"  # ğŸ“ Optional: specific folder (like partitions!)\n",
        "    \n",
        "    # ğŸ¤– AI Model Settings\n",
        "    print(\"ğŸ¤– AI model configuration...\")\n",
        "    MODEL_NAME = \"gemini-2.5-pro\"  # The LLM brain\n",
        "    MODEL_PROVIDER = \"google_vertexai\"  # Google's AI platform\n",
        "    TEMPERATURE = 0.0  # ğŸŒ¡ï¸ 0=factual, 1=creative (like randomness in ML)\n",
        "    \n",
        "    # ğŸ“„ Document Processing (like ETL transformations)\n",
        "    print(\"ğŸ“„ Document processing configuration...\")\n",
        "    CHUNK_SIZE = 1000  # ğŸ“ Characters per chunk (like batch size)\n",
        "    CHUNK_OVERLAP = 200  # ğŸ”„ Overlap between chunks (ensures continuity)\n",
        "    \n",
        "    # ğŸ—„ï¸ Vector Database Settings (like your data warehouse config)\n",
        "    print(\"ğŸ—„ï¸ Vector storage configuration...\")\n",
        "    VECTOR_STORE_PERSIST_DIRECTORY = \"./vector_store\"  # ğŸ’¾ Local storage\n",
        "    VECTOR_STORE_COLLECTION_NAME = \"documents\"  # ğŸ·ï¸ Table name equivalent\n",
        "    \n",
        "    # ğŸ§® Embedding Model (converts text â†’ numbers)\n",
        "    print(\"ğŸ§® Embedding model configuration...\")\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight & fast\n",
        "    \n",
        "    # ğŸ” Search Parameters\n",
        "    print(\"ğŸ” Search configuration...\")\n",
        "    SIMILARITY_SEARCH_K = 4  # How many relevant docs to retrieve\n",
        "    SIMILARITY_SCORE_THRESHOLD = 0.7  # Minimum relevance score\n",
        "\n",
        "# Initialize our configuration\n",
        "config = Config()\n",
        "\n",
        "print(\"\\nâœ… Configuration Complete!\")\n",
        "print(f\"ğŸ“Š Configuration Summary:\")\n",
        "print(f\"   ğŸª£ Data Source: {config.GCS_BUCKET_NAME}\")\n",
        "print(f\"   ğŸ¤– AI Model: {config.MODEL_NAME}\")\n",
        "print(f\"   ğŸ§® Embedding Model: {config.EMBEDDING_MODEL}\")\n",
        "print(f\"   ğŸ“ Chunk Size: {config.CHUNK_SIZE} characters\")\n",
        "print(f\"   ğŸ” Retrieval Count: {config.SIMILARITY_SEARCH_K} documents\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Pro Tip: These settings affect performance and accuracy!\")\n",
        "print(f\"   â€¢ Larger chunks = more context but slower processing\")\n",
        "print(f\"   â€¢ Higher K = more context but potential noise\")\n",
        "print(f\"   â€¢ Temperature 0 = factual, higher = creative\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Step 3: Building a Document Loader - Your First GenAI Component\n",
        "\n",
        "Let's build our first GenAI component! This is like creating a data ingestion pipeline, but for AI.\n",
        "\n",
        "### ğŸ” **What This Component Does:**\n",
        "1. **Connects to GCS** (like connecting to BigQuery)\n",
        "2. **Lists available documents** (like scanning table partitions)\n",
        "3. **Downloads files to local temp storage** (like caching data)\n",
        "4. **Parses different formats** (PDF, DOCX, TXT - like handling CSV, JSON, Parquet)\n",
        "5. **Adds metadata** (source tracking - like data lineage!)\n",
        "\n",
        "### ğŸ¯ **Learning Objectives:**\n",
        "- Understand document ingestion patterns in GenAI\n",
        "- See how LangChain handles different file formats\n",
        "- Learn about temporary file management\n",
        "- Practice error handling in GenAI pipelines\n",
        "\n",
        "### ğŸ’­ **Data Engineering Parallels:**\n",
        "- This is like an ETL extract phase\n",
        "- File format handling = data schema validation\n",
        "- Metadata addition = data cataloging\n",
        "- Error handling = data quality checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ—ï¸ Creating our first GenAI component - Document Loader!\n",
            "This is like building a data ingestion pipeline for AI...\n",
            "âœ… GCS Document Loader class ready!\n",
            "ğŸ’¡ This is your 'Extract' component in the GenAI ETL pipeline\n",
            "ğŸ¯ Next: We'll build the 'Transform' and 'Load' components!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“š Building Our Document Loader Class\n",
        "print(\"ğŸ—ï¸ Creating our first GenAI component - Document Loader!\")\n",
        "print(\"This is like building a data ingestion pipeline for AI...\")\n",
        "\n",
        "class GCSDocumentLoader:\n",
        "    \"\"\"\n",
        "    ğŸ“š Document Loader - Your GenAI Data Ingestion Pipeline\n",
        "    \n",
        "    This class handles the 'Extract' part of our GenAI ETL process:\n",
        "    â€¢ Connects to GCS (your data lake)\n",
        "    â€¢ Downloads documents (data ingestion)\n",
        "    â€¢ Parses different formats (schema handling)\n",
        "    â€¢ Adds metadata (data lineage)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bucket_name: str, project_id: str, folder_path: str = \"\"):\n",
        "        print(f\"ğŸ”§ Initializing Document Loader...\")\n",
        "        self.bucket_name = bucket_name\n",
        "        self.project_id = project_id\n",
        "        self.folder_path = folder_path\n",
        "        self.client = None  # Will be initialized when needed\n",
        "        print(f\"   ğŸ“ Target: gs://{bucket_name}/{folder_path}\")\n",
        "        \n",
        "    def _initialize_client(self):\n",
        "        \"\"\"ğŸ”‘ Initialize GCS client - like connecting to your data warehouse\"\"\"\n",
        "        print(\"ğŸ”‘ Connecting to Google Cloud Storage...\")\n",
        "        try:\n",
        "            # ğŸ—ï¸ Use service account authentication (secure!)\n",
        "            credentials_path = \"genai-sai-auth.json\"\n",
        "            print(f\"   ğŸ“‹ Using credentials: {credentials_path}\")\n",
        "            \n",
        "            from google.oauth2 import service_account\n",
        "            credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
        "            \n",
        "            # Initialize the GCS client\n",
        "            self.client = storage.Client(project=self.project_id, credentials=credentials)\n",
        "            print(f\"âœ… Connected to GCS project: {self.project_id}\")\n",
        "            print(f\"ğŸ” Authentication successful!\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Connection failed: {str(e)}\")\n",
        "            print(\"ğŸ’¡ Check: genai-sai-auth.json exists and has proper permissions\")\n",
        "            raise\n",
        "    \n",
        "    def list_documents(self) -> List[str]:\n",
        "        \"\"\"ğŸ“‹ Discover available documents - like scanning your data catalog\"\"\"\n",
        "        print(\"ğŸ“‹ Scanning for documents in GCS bucket...\")\n",
        "        \n",
        "        if not self.client:\n",
        "            self._initialize_client()\n",
        "        \n",
        "        assert self.client is not None, \"Failed to initialize GCS client\"\n",
        "        \n",
        "        # ğŸª£ Access the bucket (like connecting to a database)\n",
        "        bucket = self.client.bucket(self.bucket_name)\n",
        "        blobs = bucket.list_blobs(prefix=self.folder_path)\n",
        "        \n",
        "        # ğŸ” Filter for supported document types\n",
        "        document_files = []\n",
        "        supported_extensions = ['.pdf', '.docx', '.txt']\n",
        "        print(f\"   ğŸ¯ Looking for: {', '.join(supported_extensions)} files\")\n",
        "        \n",
        "        for blob in blobs:\n",
        "            if any(blob.name.lower().endswith(ext) for ext in supported_extensions):\n",
        "                document_files.append(blob.name)\n",
        "                print(f\"   ğŸ“„ Found: {blob.name}\")\n",
        "        \n",
        "        print(f\"âœ… Discovery complete: {len(document_files)} documents found\")\n",
        "        return document_files\n",
        "    \n",
        "    def download_and_load_documents(self) -> List:\n",
        "        \"\"\"\n",
        "        ğŸ“¥ Download & Parse Documents - The core ingestion process\n",
        "        \n",
        "        This is like your ETL extract + transform phases:\n",
        "        1. Download from cloud storage\n",
        "        2. Parse different file formats  \n",
        "        3. Extract text content\n",
        "        4. Add metadata for tracking\n",
        "        \"\"\"\n",
        "        print(\"ğŸ“¥ Starting document ingestion pipeline...\")\n",
        "        \n",
        "        if not self.client:\n",
        "            self._initialize_client()\n",
        "        \n",
        "        assert self.client is not None, \"Failed to initialize GCS client\"\n",
        "        \n",
        "        # Initialize containers\n",
        "        documents = []\n",
        "        bucket = self.client.bucket(self.bucket_name)\n",
        "        document_files = self.list_documents()\n",
        "        \n",
        "        if not document_files:\n",
        "            print(\"âš ï¸ No documents found to process!\")\n",
        "            return documents\n",
        "            \n",
        "        print(f\"ğŸš€ Processing {len(document_files)} documents...\")\n",
        "        \n",
        "        # Process each document\n",
        "        for i, file_name in enumerate(document_files, 1):\n",
        "            print(f\"\\nğŸ“„ Processing {i}/{len(document_files)}: {file_name}\")\n",
        "            \n",
        "            try:\n",
        "                # ğŸ’¾ Create temporary file (like staging area in ETL)\n",
        "                file_suffix = Path(file_name).suffix\n",
        "                print(f\"   ğŸ“ Creating temp file with suffix: {file_suffix}\")\n",
        "                \n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=file_suffix) as temp_file:\n",
        "                    # â¬‡ï¸ Download from GCS\n",
        "                    print(f\"   â¬‡ï¸ Downloading from GCS...\")\n",
        "                    blob = bucket.blob(file_name)\n",
        "                    blob.download_to_filename(temp_file.name)\n",
        "                    print(f\"   âœ… Downloaded to: {temp_file.name}\")\n",
        "                    \n",
        "                    # ğŸ”§ Choose appropriate parser based on file type\n",
        "                    print(f\"   ğŸ”§ Selecting parser for: {file_suffix}\")\n",
        "                    if file_name.lower().endswith('.pdf'):\n",
        "                        loader = PyPDFLoader(temp_file.name)\n",
        "                        print(\"   ğŸ“– Using PDF parser\")\n",
        "                    elif file_name.lower().endswith('.docx'):\n",
        "                        loader = Docx2txtLoader(temp_file.name)\n",
        "                        print(\"   ğŸ“ Using DOCX parser\")\n",
        "                    elif file_name.lower().endswith('.txt'):\n",
        "                        loader = TextLoader(temp_file.name)\n",
        "                        print(\"   ğŸ“„ Using TXT parser\")\n",
        "                    else:\n",
        "                        print(f\"   âš ï¸ Unsupported file type: {file_suffix}\")\n",
        "                        continue\n",
        "                    \n",
        "                    # ğŸ“– Parse the document content\n",
        "                    print(\"   ğŸ“– Parsing document content...\")\n",
        "                    doc_content = loader.load()\n",
        "                    print(f\"   ğŸ“Š Extracted {len(doc_content)} pages/sections\")\n",
        "                    \n",
        "                    # ğŸ·ï¸ Add metadata (like data lineage!)\n",
        "                    print(\"   ğŸ·ï¸ Adding metadata...\")\n",
        "                    for doc in doc_content:\n",
        "                        doc.metadata['source'] = file_name  # Original filename\n",
        "                        doc.metadata['bucket'] = self.bucket_name  # Source bucket\n",
        "                        doc.metadata['file_type'] = file_suffix  # File format\n",
        "                        doc.metadata['processed_at'] = str(os.path.getmtime(temp_file.name))\n",
        "                    \n",
        "                    documents.extend(doc_content)\n",
        "                    print(f\"   âœ… Successfully processed: {file_name}\")\n",
        "                    \n",
        "                # ğŸ§¹ Clean up temporary file (good housekeeping!)\n",
        "                os.unlink(temp_file.name)\n",
        "                print(f\"   ğŸ§¹ Cleaned up temp file\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Error processing {file_name}: {str(e)}\")\n",
        "                print(f\"   ğŸ”„ Continuing with next file...\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\nğŸ‰ Ingestion Complete!\")\n",
        "        print(f\"   ğŸ“Š Total documents processed: {len(document_files)}\")\n",
        "        print(f\"   ğŸ“„ Total text chunks extracted: {len(documents)}\")\n",
        "        print(f\"   ğŸ¯ Ready for the next pipeline stage!\")\n",
        "        \n",
        "        return documents\n",
        "\n",
        "print(\"âœ… GCS Document Loader class ready!\")\n",
        "print(\"ğŸ’¡ This is your 'Extract' component in the GenAI ETL pipeline\")\n",
        "print(\"ğŸ¯ Next: We'll build the 'Transform' and 'Load' components!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEST \n",
        "# config = Config()\n",
        "# gcsloader = GCSDocumentLoader(bucket_name=config.GCS_BUCKET_NAME, project_id=config.GOOGLE_CLOUD_PROJECT, folder_path=config.GCS_FOLDER_PATH)\n",
        "# documents = gcsloader.download_and_load_documents()\n",
        "# print(f\"âœ… Documents loaded: {len(documents)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§® Step 4: The Magic of Embeddings & Vector Search - The Heart of GenAI\n",
        "\n",
        "Now we're getting to the exciting part! Let's understand how AI actually \"understands\" your documents.\n",
        "\n",
        "### ğŸ§  **Core Concepts to Learn:**\n",
        "\n",
        "#### ğŸ“ **Text Embeddings (Text â†’ Numbers):**\n",
        "- Convert words to vectors: \"Hello\" â†’ [0.1, 0.8, -0.3, ...]\n",
        "- Similar words have similar vectors\n",
        "- Like GPS coordinates, but for meaning!\n",
        "\n",
        "#### ğŸ” **Vector Database:**\n",
        "- Stores these number representations\n",
        "- Finds \"similar\" vectors super fast\n",
        "- Like a search index, but for meaning instead of exact words\n",
        "\n",
        "#### âœ‚ï¸ **Document Chunking:**\n",
        "- Split long documents into smaller pieces\n",
        "- Balance: Big chunks = more context, Small chunks = precise answers\n",
        "- Like pagination in databases\n",
        "\n",
        "### ğŸ¯ **The RAG (Retrieval-Augmented Generation) Pattern:**\n",
        "1. **Split** documents into chunks\n",
        "2. **Convert** chunks to embeddings (vectors)\n",
        "3. **Store** in vector database\n",
        "4. **Search** for relevant chunks when user asks question\n",
        "5. **Generate** answer using retrieved context + LLM\n",
        "\n",
        "Let's build this step by step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§  Building the Semantic Search Engine - This is where the magic happens!\n",
            "We're creating the 'Transform' and 'Load' parts of our GenAI ETL pipeline...\n",
            "âœ… Semantic Search Engine class complete!\n",
            "ğŸ‰ You've just built a complete RAG (Retrieval-Augmented Generation) system!\n",
            "ğŸ’¡ This combines the best of search engines and AI language models\n",
            "ğŸ¯ Next: Let's put it all together and test it!\n"
          ]
        }
      ],
      "source": [
        "# ğŸ§® Building the Semantic Search Engine - The Brain of Our System\n",
        "print(\"ğŸ§  Building the Semantic Search Engine - This is where the magic happens!\")\n",
        "print(\"We're creating the 'Transform' and 'Load' parts of our GenAI ETL pipeline...\")\n",
        "\n",
        "class SemanticSearchEngine:\n",
        "    \"\"\"\n",
        "    ğŸ§® Semantic Search Engine - The GenAI Processing Brain\n",
        "    \n",
        "    This class handles the core GenAI transformations:\n",
        "    â€¢ Text chunking (data preprocessing)\n",
        "    â€¢ Embeddings generation (feature engineering) \n",
        "    â€¢ Vector storage (specialized database)\n",
        "    â€¢ Similarity search (intelligent querying)\n",
        "    â€¢ Question answering (the final output)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        print(\"ğŸ”§ Initializing Semantic Search Engine...\")\n",
        "        self.config = config\n",
        "        \n",
        "        # Initialize all components as None (lazy loading)\n",
        "        self.embeddings = None      # Text â†’ Vector converter\n",
        "        self.vector_store = None    # Vector database\n",
        "        self.retriever = None       # Search interface\n",
        "        self.model = None          # LLM (Gemini 2.5 Pro)\n",
        "        self.qa_chain = None       # Complete QA pipeline\n",
        "        \n",
        "        print(\"   ğŸ“Š Engine initialized with lazy loading pattern\")\n",
        "        print(\"   ğŸ¯ Components will be created when needed\")\n",
        "        \n",
        "    def initialize_embeddings(self):\n",
        "        \"\"\"\n",
        "        ğŸ§® Initialize Embedding Model - Converting Text to Vectors\n",
        "        \n",
        "        This is like feature engineering in ML:\n",
        "        â€¢ Converts text to numerical vectors\n",
        "        â€¢ Captures semantic meaning\n",
        "        â€¢ Enables similarity calculations\n",
        "        \"\"\"\n",
        "        print(\"ğŸ§® Setting up embedding model...\")\n",
        "        print(\"   ğŸ“ This converts text to vectors (numbers that capture meaning)\")\n",
        "        \n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=self.config.EMBEDDING_MODEL,\n",
        "            model_kwargs={'device': 'cpu'}  # Use CPU for compatibility\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ… Embedding model ready: {self.config.EMBEDDING_MODEL}\")\n",
        "        print(\"   ğŸ’¡ Now we can convert any text to vectors!\")\n",
        "        print(\"   ğŸ“Š Example: 'Hello' â†’ [0.1, 0.8, -0.3, ...384 numbers]\")\n",
        "    \n",
        "    def process_documents(self, documents):\n",
        "        \"\"\"\n",
        "        âœ‚ï¸ Document Chunking - Breaking Text into Digestible Pieces\n",
        "        \n",
        "        This is like data preprocessing:\n",
        "        â€¢ Split long documents into smaller chunks\n",
        "        â€¢ Maintain overlap for context continuity\n",
        "        â€¢ Optimize for both accuracy and performance\n",
        "        \"\"\"\n",
        "        print(\"âœ‚ï¸ Starting document chunking process...\")\n",
        "        print(\"   ğŸ“ Breaking documents into optimal-sized pieces\")\n",
        "        \n",
        "        # Create the text splitter with intelligent chunking\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.config.CHUNK_SIZE,        # Max characters per chunk\n",
        "            chunk_overlap=self.config.CHUNK_OVERLAP,  # Overlap between chunks\n",
        "            length_function=len,                      # How to measure length\n",
        "        )\n",
        "        \n",
        "        print(f\"   âš™ï¸ Chunking parameters:\")\n",
        "        print(f\"      ğŸ“ Chunk size: {self.config.CHUNK_SIZE} characters\") \n",
        "        print(f\"      ğŸ”„ Overlap: {self.config.CHUNK_OVERLAP} characters\")\n",
        "        print(f\"   ğŸ”„ Processing {len(documents)} documents...\")\n",
        "        \n",
        "        # Split the documents\n",
        "        split_documents = text_splitter.split_documents(documents)\n",
        "        \n",
        "        # Show some statistics\n",
        "        avg_chunk_size = sum(len(doc.page_content) for doc in split_documents) / len(split_documents)\n",
        "        \n",
        "        print(f\"âœ… Chunking complete!\")\n",
        "        print(f\"   ğŸ“Š Input: {len(documents)} documents\")\n",
        "        print(f\"   ğŸ“„ Output: {len(split_documents)} chunks\")\n",
        "        print(f\"   ğŸ“ Average chunk size: {avg_chunk_size:.0f} characters\")\n",
        "        print(f\"   ğŸ¯ Ready for embedding generation!\")\n",
        "        \n",
        "        return split_documents\n",
        "    \n",
        "    def create_vector_store(self, documents):\n",
        "        \"\"\"\n",
        "        ğŸ—„ï¸ Create Vector Database - The Heart of Semantic Search\n",
        "        \n",
        "        This is the 'Load' phase of our GenAI ETL:\n",
        "        â€¢ Convert text chunks to embeddings\n",
        "        â€¢ Store in specialized vector database\n",
        "        â€¢ Create search interface for similarity queries\n",
        "        \"\"\"\n",
        "        print(\"ğŸ—„ï¸ Creating vector database...\")\n",
        "        print(\"   ğŸ§® This will convert all text to vectors and store them\")\n",
        "        \n",
        "        # Initialize embeddings if not already done\n",
        "        if not self.embeddings:\n",
        "            print(\"   ğŸ”§ Embedding model not initialized, setting it up...\")\n",
        "            self.initialize_embeddings()\n",
        "            \n",
        "        print(\"   ğŸ“Š Converting documents to vectors...\")\n",
        "        print(\"   â³ This might take a moment - we're doing math on every piece of text!\")\n",
        "        \n",
        "        # Create ChromaDB vector store\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=self.config.VECTOR_STORE_COLLECTION_NAME,\n",
        "            persist_directory=self.config.VECTOR_STORE_PERSIST_DIRECTORY\n",
        "        )\n",
        "        \n",
        "        print(\"   ğŸ” Setting up similarity search interface...\")\n",
        "        \n",
        "        # Create retriever for similarity search\n",
        "        self.retriever = self.vector_store.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": self.config.SIMILARITY_SEARCH_K}\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ… Vector store created successfully!\")\n",
        "        print(f\"   ğŸ“Š Stored {len(documents)} document chunks as vectors\")\n",
        "        print(f\"   ğŸ” Search configured to return top {self.config.SIMILARITY_SEARCH_K} matches\")\n",
        "        print(f\"   ğŸ’¾ Persisted to: {self.config.VECTOR_STORE_PERSIST_DIRECTORY}\")\n",
        "        print(f\"   ğŸ¯ Ready for semantic search!\")\n",
        "        \n",
        "        return self.vector_store\n",
        "    \n",
        "    def initialize_model(self):\n",
        "        \"\"\"\n",
        "        ğŸ¤– Initialize Gemini 2.5 Pro - The Language Understanding Brain\n",
        "        \n",
        "        This sets up our Large Language Model:\n",
        "        â€¢ Connects to Google's Gemini 2.5 Pro\n",
        "        â€¢ Configures response style (temperature)\n",
        "        â€¢ Prepares for question answering\n",
        "        \"\"\"\n",
        "        print(\"ğŸ¤– Connecting to Gemini 2.5 Pro...\")\n",
        "        print(\"   ğŸ§  This is Google's most advanced language model\")\n",
        "        \n",
        "        self.model = init_chat_model(\n",
        "            model=self.config.MODEL_NAME,\n",
        "            model_provider=self.config.MODEL_PROVIDER,\n",
        "            temperature=self.config.TEMPERATURE\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ… Gemini 2.5 Pro connected successfully!\")\n",
        "        print(f\"   ğŸŒ¡ï¸ Temperature: {self.config.TEMPERATURE} (0=factual, 1=creative)\")\n",
        "        print(f\"   ğŸ¯ Ready to answer questions!\")\n",
        "        \n",
        "        return self.model\n",
        "    \n",
        "    def create_qa_chain(self):\n",
        "        \"\"\"\n",
        "        ğŸ”— Create Question-Answering Pipeline - Putting It All Together\n",
        "        \n",
        "        This combines everything into a complete RAG system:\n",
        "        â€¢ Retrieval: Find relevant documents\n",
        "        â€¢ Augmentation: Add context to the question  \n",
        "        â€¢ Generation: Generate answer using LLM\n",
        "        \"\"\"\n",
        "        print(\"ğŸ”— Building Question-Answering pipeline...\")\n",
        "        print(\"   ğŸ¯ This combines search + AI to answer questions\")\n",
        "        \n",
        "        # Initialize model if needed\n",
        "        if not self.model:\n",
        "            print(\"   ğŸ¤– LLM not initialized, setting it up...\")\n",
        "            self.initialize_model()\n",
        "            \n",
        "        if not self.retriever:\n",
        "            raise ValueError(\"âŒ Vector store must be created first!\")\n",
        "        \n",
        "        print(\"   ğŸ“ Creating prompt template...\")\n",
        "        \n",
        "        # Design the prompt template (how we ask the AI)\n",
        "        prompt_template = \"\"\"You are a helpful AI assistant. Use the following pieces of context to answer the question. \n",
        "        \n",
        "If you don't know the answer based on the provided context, say that you don't know. Don't make up information.\n",
        "\n",
        "Context Information:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "        \n",
        "        prompt = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "        \n",
        "        print(\"   ğŸ”— Connecting all components...\")\n",
        "        \n",
        "        # Create the complete RAG chain\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.model,                           # Language model\n",
        "            chain_type=\"stuff\",                       # How to combine docs\n",
        "            retriever=self.retriever,                 # Document retriever\n",
        "            chain_type_kwargs={\"prompt\": prompt},     # Custom prompt\n",
        "            return_source_documents=True              # Include source info\n",
        "        )\n",
        "        \n",
        "        print(\"âœ… Question-Answering pipeline ready!\")\n",
        "        print(\"   ğŸ” Can now search your documents\")\n",
        "        print(\"   ğŸ¤– Can answer questions using AI\") \n",
        "        print(\"   ğŸ“š Will show source documents\")\n",
        "        print(\"   ğŸ‰ Your GenAI system is complete!\")\n",
        "        \n",
        "        return self.qa_chain\n",
        "    \n",
        "    def search_documents(self, query: str, k: int = 4):\n",
        "        \"\"\"\n",
        "        ğŸ” Semantic Document Search - Find Similar Content\n",
        "        \n",
        "        This performs similarity search without AI generation:\n",
        "        â€¢ Convert query to vector\n",
        "        â€¢ Find most similar document chunks\n",
        "        â€¢ Return ranked results\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"âŒ Vector store must be created first!\")\n",
        "            \n",
        "        k = k if k > 0 else self.config.SIMILARITY_SEARCH_K\n",
        "        print(f\"ğŸ” Searching for documents similar to: '{query}'\")\n",
        "        print(f\"   ğŸ“Š Looking for top {k} matches...\")\n",
        "        \n",
        "        results = self.vector_store.similarity_search(query, k=k)\n",
        "        \n",
        "        print(f\"âœ… Found {len(results)} similar documents\")\n",
        "        return results\n",
        "    \n",
        "    def ask_question(self, question: str):\n",
        "        \"\"\"\n",
        "        ğŸ¤” Ask Questions About Your Documents - The Complete RAG Experience\n",
        "        \n",
        "        This is the full RAG (Retrieval-Augmented Generation) process:\n",
        "        1. Find relevant documents (Retrieval)\n",
        "        2. Add them as context (Augmentation)  \n",
        "        3. Generate answer with AI (Generation)\n",
        "        \"\"\"\n",
        "        if not self.qa_chain:\n",
        "            raise ValueError(\"âŒ QA chain must be created first!\")\n",
        "        \n",
        "        print(f\"ğŸ¤” Processing question: '{question}'\")\n",
        "        print(\"ğŸ” Step 1: Searching for relevant information...\")\n",
        "        print(\"ğŸ¤– Step 2: Generating AI response...\")\n",
        "        print(\"ğŸ“š Step 3: Compiling sources...\")\n",
        "        \n",
        "        # Run the complete RAG pipeline\n",
        "        result = self.qa_chain({\"query\": question})\n",
        "        \n",
        "        answer = result[\"result\"]\n",
        "        source_docs = result[\"source_documents\"]\n",
        "        \n",
        "        print(f\"\\nâœ… Question answering complete!\")\n",
        "        print(f\"ğŸ’¡ Answer generated using {len(source_docs)} source documents\")\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"source_documents\": source_docs\n",
        "        }\n",
        "\n",
        "print(\"âœ… Semantic Search Engine class complete!\")\n",
        "print(\"ğŸ‰ You've just built a complete RAG (Retrieval-Augmented Generation) system!\")\n",
        "print(\"ğŸ’¡ This combines the best of search engines and AI language models\")\n",
        "print(\"ğŸ¯ Next: Let's put it all together and test it!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Step 5: Putting It All Together - Building Your First GenAI Application!\n",
        "\n",
        "This is the exciting part! We'll now assemble all our components into a working GenAI system.\n",
        "\n",
        "### ğŸ¯ **What We're About to Do:**\n",
        "1. **Load documents** from your GCS bucket (Extract)\n",
        "2. **Process and chunk** the text (Transform)  \n",
        "3. **Create vector embeddings** (Feature Engineering)\n",
        "4. **Build the vector database** (Load)\n",
        "5. **Initialize the AI model** (Deploy)\n",
        "6. **Create the QA pipeline** (Production Ready!)\n",
        "\n",
        "### ğŸ’¡ **Learning Focus:**\n",
        "- See how all GenAI components work together\n",
        "- Understand the complete data flow\n",
        "- Watch the RAG pipeline in action\n",
        "- Learn about performance considerations\n",
        "\n",
        "### ğŸ”— **The GenAI Pipeline Flow:**\n",
        "```\n",
        "Documents (GCS) â†’ Chunking â†’ Embeddings â†’ Vector DB â†’ Retrieval â†’ LLM â†’ Answer\n",
        "```\n",
        "\n",
        "Ready to see your GenAI system come to life? Let's go!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ‰ Welcome to GenAI System Assembly!\n",
            "Let's build your semantic search engine step by step...\n",
            "ğŸ¯ This is like deploying a complete data pipeline, but for AI!\n",
            "\n",
            "ğŸ”§ INITIALIZATION ğŸ”§\n",
            "Creating the main engine instance...\n",
            "ğŸ”§ Initializing Semantic Search Engine...\n",
            "   ğŸ“Š Engine initialized with lazy loading pattern\n",
            "   ğŸ¯ Components will be created when needed\n",
            "\n",
            "============================================================\n",
            "ğŸ“š STEP 1: DOCUMENT INGESTION (Extract Phase)\n",
            "============================================================\n",
            "ğŸ¯ Learning Goal: Understand how GenAI systems ingest data\n",
            "ğŸ’¡ This is like your ETL extract phase - getting data from source\n",
            "\n",
            "ğŸª£ Connecting to GCS bucket: genai-sai-bucket\n",
            "ğŸ”§ Initializing Document Loader...\n",
            "   ğŸ“ Target: gs://genai-sai-bucket/\n",
            "\n",
            "ğŸ“¥ Starting document ingestion...\n",
            "â³ This process will:\n",
            "   1. Connect to your GCS bucket\n",
            "   2. Scan for supported file types\n",
            "   3. Download and parse each document\n",
            "   4. Extract text content\n",
            "   5. Add metadata for tracking\n",
            "ğŸ“¥ Starting document ingestion pipeline...\n",
            "ğŸ”‘ Connecting to Google Cloud Storage...\n",
            "   ğŸ“‹ Using credentials: genai-sai-auth.json\n",
            "âœ… Connected to GCS project: igneous-future-451513-v8d\n",
            "ğŸ” Authentication successful!\n",
            "ğŸ“‹ Scanning for documents in GCS bucket...\n",
            "   ğŸ¯ Looking for: .pdf, .docx, .txt files\n",
            "   ğŸ“„ Found: Metro AG â€“ Wikipedia.pdf\n",
            "âœ… Discovery complete: 1 documents found\n",
            "ğŸš€ Processing 1 documents...\n",
            "\n",
            "ğŸ“„ Processing 1/1: Metro AG â€“ Wikipedia.pdf\n",
            "   ğŸ“ Creating temp file with suffix: .pdf\n",
            "   â¬‡ï¸ Downloading from GCS...\n",
            "   âœ… Downloaded to: /var/folders/7x/3vpj1fqx717cs602xgtbss700000gn/T/tmpn8ysxp60.pdf\n",
            "   ğŸ”§ Selecting parser for: .pdf\n",
            "   ğŸ“– Using PDF parser\n",
            "   ğŸ“– Parsing document content...\n",
            "   ğŸ“Š Extracted 18 pages/sections\n",
            "   ğŸ·ï¸ Adding metadata...\n",
            "   âœ… Successfully processed: Metro AG â€“ Wikipedia.pdf\n",
            "   ğŸ§¹ Cleaned up temp file\n",
            "\n",
            "ğŸ‰ Ingestion Complete!\n",
            "   ğŸ“Š Total documents processed: 1\n",
            "   ğŸ“„ Total text chunks extracted: 18\n",
            "   ğŸ¯ Ready for the next pipeline stage!\n",
            "\n",
            "ğŸ“Š INGESTION RESULTS:\n",
            "   âœ… Documents loaded: 18\n",
            "   ğŸ“„ Sample sources: ['Metro AG â€“ Wikipedia.pdf']\n",
            "   ğŸ“ Total text characters: 60,710\n",
            "   ğŸ’¾ Average document size: 3,372 characters\n",
            "\n",
            "ğŸ’¡ What just happened:\n",
            "   â€¢ Your documents are now in memory as text objects\n",
            "   â€¢ Each has metadata (source file, bucket info)\n",
            "   â€¢ Ready for the next stage: chunking!\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ BUILDING YOUR GENAI SYSTEM - Step by Step!\n",
        "print(\"ğŸ‰ Welcome to GenAI System Assembly!\")\n",
        "print(\"Let's build your semantic search engine step by step...\")\n",
        "print(\"ğŸ¯ This is like deploying a complete data pipeline, but for AI!\")\n",
        "\n",
        "print(\"\\n\" + \"ğŸ”§\" + \" INITIALIZATION \" + \"ğŸ”§\")\n",
        "print(\"Creating the main engine instance...\")\n",
        "search_engine = SemanticSearchEngine(config)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“š STEP 1: DOCUMENT INGESTION (Extract Phase)\")\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ¯ Learning Goal: Understand how GenAI systems ingest data\")\n",
        "print(\"ğŸ’¡ This is like your ETL extract phase - getting data from source\")\n",
        "\n",
        "print(f\"\\nğŸª£ Connecting to GCS bucket: {config.GCS_BUCKET_NAME}\")\n",
        "document_loader = GCSDocumentLoader(\n",
        "    bucket_name=config.GCS_BUCKET_NAME,\n",
        "    project_id=config.GOOGLE_CLOUD_PROJECT,\n",
        "    folder_path=config.GCS_FOLDER_PATH\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“¥ Starting document ingestion...\")\n",
        "print(\"â³ This process will:\")\n",
        "print(\"   1. Connect to your GCS bucket\")\n",
        "print(\"   2. Scan for supported file types\")\n",
        "print(\"   3. Download and parse each document\")\n",
        "print(\"   4. Extract text content\")\n",
        "print(\"   5. Add metadata for tracking\")\n",
        "\n",
        "documents = document_loader.download_and_load_documents()\n",
        "\n",
        "print(f\"\\nğŸ“Š INGESTION RESULTS:\")\n",
        "print(f\"   âœ… Documents loaded: {len(documents)}\")\n",
        "if documents:\n",
        "    sources = list(set([doc.metadata.get('source', 'Unknown') for doc in documents[:3]]))\n",
        "    print(f\"   ğŸ“„ Sample sources: {sources}\")\n",
        "    total_chars = sum(len(doc.page_content) for doc in documents)\n",
        "    print(f\"   ğŸ“ Total text characters: {total_chars:,}\")\n",
        "    print(f\"   ğŸ’¾ Average document size: {total_chars//len(documents):,} characters\")\n",
        "else:\n",
        "    print(\"   âš ï¸ No documents found - check your bucket configuration!\")\n",
        "    \n",
        "print(f\"\\nğŸ’¡ What just happened:\")\n",
        "print(f\"   â€¢ Your documents are now in memory as text objects\")\n",
        "print(f\"   â€¢ Each has metadata (source file, bucket info)\")\n",
        "print(f\"   â€¢ Ready for the next stage: chunking!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "âœ‚ï¸ STEP 2: DOCUMENT PROCESSING (Transform Phase)\n",
            "============================================================\n",
            "ğŸ¯ Learning Goal: Understand text chunking and why it matters\n",
            "ğŸ’¡ This is like data preprocessing - preparing data for optimal AI processing\n",
            "\n",
            "ğŸ“ About to process 18 documents\n",
            "ğŸ” Why do we need chunking?\n",
            "   â€¢ AI models have context limits (like memory constraints)\n",
            "   â€¢ Smaller chunks = more precise retrieval\n",
            "   â€¢ Overlap ensures we don't lose context at boundaries\n",
            "\n",
            "âš™ï¸ Chunking configuration:\n",
            "   ğŸ“ Chunk size: 1000 characters\n",
            "   ğŸ”„ Overlap: 200 characters\n",
            "   ğŸ’¡ This balances precision vs context!\n",
            "âœ‚ï¸ Starting document chunking process...\n",
            "   ğŸ“ Breaking documents into optimal-sized pieces\n",
            "   âš™ï¸ Chunking parameters:\n",
            "      ğŸ“ Chunk size: 1000 characters\n",
            "      ğŸ”„ Overlap: 200 characters\n",
            "   ğŸ”„ Processing 18 documents...\n",
            "âœ… Chunking complete!\n",
            "   ğŸ“Š Input: 18 documents\n",
            "   ğŸ“„ Output: 81 chunks\n",
            "   ğŸ“ Average chunk size: 879 characters\n",
            "   ğŸ¯ Ready for embedding generation!\n",
            "\n",
            "============================================================\n",
            "ğŸ§® STEP 3: VECTOR EMBEDDING CREATION (Feature Engineering)\n",
            "============================================================\n",
            "ğŸ¯ Learning Goal: Understand how text becomes searchable vectors\n",
            "ğŸ’¡ This converts human language to mathematical representations\n",
            "\n",
            "ğŸ”® The embedding magic:\n",
            "   ğŸ“ 'Machine learning' â†’ [0.2, -0.1, 0.8, ...384 numbers]\n",
            "   ğŸ“ 'AI algorithms' â†’ [0.3, -0.2, 0.7, ...384 numbers]\n",
            "   ğŸ¯ Similar concepts get similar numbers!\n",
            "\n",
            "ğŸ—„ï¸ Creating vector database with 81 chunks...\n",
            "â³ This will take a moment - we're doing math on every piece of text!\n",
            "ğŸ—„ï¸ Creating vector database...\n",
            "   ğŸ§® This will convert all text to vectors and store them\n",
            "   ğŸ”§ Embedding model not initialized, setting it up...\n",
            "ğŸ§® Setting up embedding model...\n",
            "   ğŸ“ This converts text to vectors (numbers that capture meaning)\n",
            "âœ… Embedding model ready: sentence-transformers/all-MiniLM-L6-v2\n",
            "   ğŸ’¡ Now we can convert any text to vectors!\n",
            "   ğŸ“Š Example: 'Hello' â†’ [0.1, 0.8, -0.3, ...384 numbers]\n",
            "   ğŸ“Š Converting documents to vectors...\n",
            "   â³ This might take a moment - we're doing math on every piece of text!\n",
            "   ğŸ” Setting up similarity search interface...\n",
            "âœ… Vector store created successfully!\n",
            "   ğŸ“Š Stored 81 document chunks as vectors\n",
            "   ğŸ” Search configured to return top 4 matches\n",
            "   ğŸ’¾ Persisted to: ./vector_store\n",
            "   ğŸ¯ Ready for semantic search!\n",
            "\n",
            "ğŸ“Š PROCESSING COMPLETE!\n",
            "   ğŸ“š Original documents: 18\n",
            "   âœ‚ï¸ Text chunks created: 81\n",
            "   ğŸ§® Vector embeddings: 81 (one per chunk)\n",
            "   ğŸ—„ï¸ Vector database: âœ… Ready for semantic search!\n",
            "\n",
            "ğŸ’¡ What you now have:\n",
            "   â€¢ Every piece of text is now a vector in all-MiniLM-L6-v2\n",
            "   â€¢ You can find 'similar' content mathematically\n",
            "   â€¢ The AI can search by meaning, not just keywords!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ‚ï¸ STEP 2: DOCUMENT PROCESSING (Transform Phase)\")\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ¯ Learning Goal: Understand text chunking and why it matters\")\n",
        "print(\"ğŸ’¡ This is like data preprocessing - preparing data for optimal AI processing\")\n",
        "\n",
        "if documents:\n",
        "    print(f\"\\nğŸ“ About to process {len(documents)} documents\")\n",
        "    print(\"ğŸ” Why do we need chunking?\")\n",
        "    print(\"   â€¢ AI models have context limits (like memory constraints)\")\n",
        "    print(\"   â€¢ Smaller chunks = more precise retrieval\")\n",
        "    print(\"   â€¢ Overlap ensures we don't lose context at boundaries\")\n",
        "    \n",
        "    print(f\"\\nâš™ï¸ Chunking configuration:\")\n",
        "    print(f\"   ğŸ“ Chunk size: {config.CHUNK_SIZE} characters\")\n",
        "    print(f\"   ğŸ”„ Overlap: {config.CHUNK_OVERLAP} characters\")\n",
        "    print(f\"   ğŸ’¡ This balances precision vs context!\")\n",
        "    \n",
        "    # Process documents (split into chunks)\n",
        "    processed_docs = search_engine.process_documents(documents)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ§® STEP 3: VECTOR EMBEDDING CREATION (Feature Engineering)\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ¯ Learning Goal: Understand how text becomes searchable vectors\")\n",
        "    print(\"ğŸ’¡ This converts human language to mathematical representations\")\n",
        "    \n",
        "    print(\"\\nğŸ”® The embedding magic:\")\n",
        "    print(\"   ğŸ“ 'Machine learning' â†’ [0.2, -0.1, 0.8, ...384 numbers]\")\n",
        "    print(\"   ğŸ“ 'AI algorithms' â†’ [0.3, -0.2, 0.7, ...384 numbers]\")\n",
        "    print(\"   ğŸ¯ Similar concepts get similar numbers!\")\n",
        "    \n",
        "    print(f\"\\nğŸ—„ï¸ Creating vector database with {len(processed_docs)} chunks...\")\n",
        "    print(\"â³ This will take a moment - we're doing math on every piece of text!\")\n",
        "    \n",
        "    # Create vector store\n",
        "    vector_store = search_engine.create_vector_store(processed_docs)\n",
        "    \n",
        "    print(f\"\\nğŸ“Š PROCESSING COMPLETE!\")\n",
        "    print(f\"   ğŸ“š Original documents: {len(documents)}\")\n",
        "    print(f\"   âœ‚ï¸ Text chunks created: {len(processed_docs)}\")\n",
        "    print(f\"   ğŸ§® Vector embeddings: {len(processed_docs)} (one per chunk)\")\n",
        "    print(f\"   ğŸ—„ï¸ Vector database: âœ… Ready for semantic search!\")\n",
        "    \n",
        "    print(f\"\\nğŸ’¡ What you now have:\")\n",
        "    print(f\"   â€¢ Every piece of text is now a vector in {config.EMBEDDING_MODEL.split('/')[-1]}\")\n",
        "    print(f\"   â€¢ You can find 'similar' content mathematically\")\n",
        "    print(f\"   â€¢ The AI can search by meaning, not just keywords!\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No documents found. Please check your GCS bucket and configuration.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ¤– STEP 4: AI MODEL INTEGRATION (The Brain)\n",
            "============================================================\n",
            "ğŸ¯ Learning Goal: Understand how LLMs integrate with search\n",
            "ğŸ’¡ This connects Google's most advanced AI to your data\n",
            "\n",
            "ğŸ§  Connecting to gemini-2.5-pro...\n",
            "ğŸŒŸ What makes Gemini 2.5 Pro special:\n",
            "   â€¢ Understands context and nuance\n",
            "   â€¢ Can reason across multiple documents\n",
            "   â€¢ Generates human-like responses\n",
            "   â€¢ Knows when it doesn't know something\n",
            "\n",
            "ğŸ”— Building the complete RAG pipeline...\n",
            "ğŸ“‹ RAG = Retrieval-Augmented Generation\n",
            "   1. ğŸ” Retrieve: Find relevant documents\n",
            "   2. ğŸ“ Augment: Add context to the question\n",
            "   3. ğŸ¤– Generate: Create AI-powered answer\n",
            "ğŸ”— Building Question-Answering pipeline...\n",
            "   ğŸ¯ This combines search + AI to answer questions\n",
            "   ğŸ¤– LLM not initialized, setting it up...\n",
            "ğŸ¤– Connecting to Gemini 2.5 Pro...\n",
            "   ğŸ§  This is Google's most advanced language model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Gemini 2.5 Pro connected successfully!\n",
            "   ğŸŒ¡ï¸ Temperature: 0.0 (0=factual, 1=creative)\n",
            "   ğŸ¯ Ready to answer questions!\n",
            "   ğŸ“ Creating prompt template...\n",
            "   ğŸ”— Connecting all components...\n",
            "âœ… Question-Answering pipeline ready!\n",
            "   ğŸ” Can now search your documents\n",
            "   ğŸ¤– Can answer questions using AI\n",
            "   ğŸ“š Will show source documents\n",
            "   ğŸ‰ Your GenAI system is complete!\n",
            "\n",
            "ğŸ‰ YOUR GENAI SYSTEM IS READY!\n",
            "========================================\n",
            "   ğŸ¤– AI Model: gemini-2.5-pro âœ…\n",
            "   ğŸ“š Document Store: 81 chunks âœ…\n",
            "   ğŸ§® Vector Database: ChromaDB âœ…\n",
            "   ğŸ”— QA Pipeline: Complete RAG system âœ…\n",
            "   ğŸ¯ Ready to answer questions! ğŸš€\n",
            "\n",
            "ğŸ’¡ What you've built:\n",
            "   â€¢ A complete GenAI application from scratch\n",
            "   â€¢ Enterprise-grade document search system\n",
            "   â€¢ AI that can understand and answer questions about YOUR data\n",
            "   â€¢ Production-ready RAG (Retrieval-Augmented Generation) pipeline\n",
            "\n",
            "ğŸš€ Next: Let's test your creation!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¤– STEP 4: AI MODEL INTEGRATION (The Brain)\")\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ¯ Learning Goal: Understand how LLMs integrate with search\")\n",
        "print(\"ğŸ’¡ This connects Google's most advanced AI to your data\")\n",
        "\n",
        "if search_engine.vector_store:\n",
        "    print(f\"\\nğŸ§  Connecting to {config.MODEL_NAME}...\")\n",
        "    print(\"ğŸŒŸ What makes Gemini 2.5 Pro special:\")\n",
        "    print(\"   â€¢ Understands context and nuance\")\n",
        "    print(\"   â€¢ Can reason across multiple documents\")\n",
        "    print(\"   â€¢ Generates human-like responses\")\n",
        "    print(\"   â€¢ Knows when it doesn't know something\")\n",
        "    \n",
        "    print(f\"\\nğŸ”— Building the complete RAG pipeline...\")\n",
        "    print(\"ğŸ“‹ RAG = Retrieval-Augmented Generation\")\n",
        "    print(\"   1. ğŸ” Retrieve: Find relevant documents\")\n",
        "    print(\"   2. ğŸ“ Augment: Add context to the question\")\n",
        "    print(\"   3. ğŸ¤– Generate: Create AI-powered answer\")\n",
        "    \n",
        "    # Initialize the model and create QA chain\n",
        "    qa_chain = search_engine.create_qa_chain()\n",
        "    \n",
        "    print(f\"\\nğŸ‰ YOUR GENAI SYSTEM IS READY!\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"   ğŸ¤– AI Model: {config.MODEL_NAME} âœ…\")\n",
        "    print(f\"   ğŸ“š Document Store: {len(processed_docs)} chunks âœ…\") \n",
        "    print(f\"   ğŸ§® Vector Database: ChromaDB âœ…\")\n",
        "    print(f\"   ğŸ”— QA Pipeline: Complete RAG system âœ…\")\n",
        "    print(f\"   ğŸ¯ Ready to answer questions! ğŸš€\")\n",
        "    \n",
        "    print(f\"\\nğŸ’¡ What you've built:\")\n",
        "    print(f\"   â€¢ A complete GenAI application from scratch\")\n",
        "    print(f\"   â€¢ Enterprise-grade document search system\")\n",
        "    print(f\"   â€¢ AI that can understand and answer questions about YOUR data\")\n",
        "    print(f\"   â€¢ Production-ready RAG (Retrieval-Augmented Generation) pipeline\")\n",
        "    \n",
        "    print(f\"\\nğŸš€ Next: Let's test your creation!\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ Vector store not available. Cannot create QA chain.\")\n",
        "    print(\"ğŸ’¡ Make sure Step 2 completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª Step 6: Testing Your GenAI System - See the Magic in Action!\n",
        "\n",
        "Time to test your creation! This is where you'll see the power of GenAI firsthand.\n",
        "\n",
        "### ğŸ¯ **What You'll Learn by Testing:**\n",
        "- How semantic search finds relevant content\n",
        "- How AI generates answers from your documents  \n",
        "- The importance of source attribution\n",
        "- How to evaluate GenAI system performance\n",
        "\n",
        "### ğŸ” **Types of Questions to Try:**\n",
        "1. **Factual**: \"What is...?\" \"When did...?\" \"How many...?\"\n",
        "2. **Analytical**: \"Why does...?\" \"What are the implications...?\"\n",
        "3. **Comparative**: \"How does X compare to Y?\" \n",
        "4. **Summary**: \"Summarize the main points about...\"\n",
        "5. **Exploratory**: \"What does the document say about...?\"\n",
        "\n",
        "### ğŸ’¡ **Understanding the Results:**\n",
        "- **Answer**: AI-generated response based on your documents\n",
        "- **Sources**: Which document chunks were used (like data lineage!)\n",
        "- **Confidence**: How well the retrieved documents match your question\n",
        "\n",
        "Let's start with an automated test to see your system in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª Testing Semantic Search Engine\n",
            "==================================================\n",
            "\n",
            "ğŸ“ Sample Question: What is the main topic discussed in the documents?\n",
            "--------------------------------------------------\n",
            "ğŸ¤” Processing question: 'What is the main topic discussed in the documents?'\n",
            "ğŸ” Step 1: Searching for relevant information...\n",
            "ğŸ¤– Step 2: Generating AI response...\n",
            "ğŸ“š Step 3: Compiling sources...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Question answering complete!\n",
            "ğŸ’¡ Answer generated using 4 source documents\n",
            "\n",
            "ğŸ’¡ Answer:\n",
            "Based on the provided context, the main topic discussed in the documents is the company **Metro AG**.\n",
            "\n",
            "The text is a list of sources, the majority of which are annual reports (\"GeschÃ¤ftsberichte\") from Metro AG for various years. Other sources mentioned relate to Metro AG's share ownership, corporate structure, and legal cases.\n",
            "\n",
            "ğŸ“š Source Documents:\n",
            "   1. Metro AG â€“ Wikipedia.pdf (Preview: GeschÃ¤ftsfÃ¼hrung, 11.Â April 2025, abgerufen am 17.Â April 2025.\n",
            "100. Reuters: Investor Kretinsky kont...)\n",
            "   2. Metro AG â€“ Wikipedia.pdf (Preview: 103. GeschÃ¤ftsbericht 2018/19. (https://berichte.metroag.de/geschaeftsbericht/2018-2019/servicesei\n",
            "t...)\n",
            "   3. Metro AG â€“ Wikipedia.pdf (Preview: 36. Die GeschÃ¤ftsbereiche der Metro-Gruppe (https://web.archive.org/web/20080330120141/http://\n",
            "www.n...)\n",
            "   4. Metro AG â€“ Wikipedia.pdf (Preview: 107. GeschÃ¤ftsbericht 2022/23. (https://berichte.metroag.de/geschaeftsbericht/2022-2023/_assets/d\n",
            "ow...)\n",
            "\n",
            "ğŸ’¡ Try other questions:\n",
            "   2. Can you summarize the key points?\n",
            "   3. What are the main conclusions or findings?\n",
            "   4. Are there any specific recommendations mentioned?\n",
            "   5. What methodology or approach is described?\n"
          ]
        }
      ],
      "source": [
        "# Test the semantic search engine with sample questions\n",
        "def test_search_engine():\n",
        "    \"\"\"Test function to demonstrate the search engine capabilities.\"\"\"\n",
        "    \n",
        "    if not search_engine.qa_chain:\n",
        "        print(\"âŒ QA chain not available. Please run the setup cells first.\")\n",
        "        return\n",
        "    \n",
        "    # Sample questions - modify these based on your documents\n",
        "    sample_questions = [\n",
        "        \"What is the main topic discussed in the documents?\",\n",
        "        \"Can you summarize the key points?\",\n",
        "        \"What are the main conclusions or findings?\",\n",
        "        \"Are there any specific recommendations mentioned?\",\n",
        "        \"What methodology or approach is described?\"\n",
        "    ]\n",
        "    \n",
        "    print(\"ğŸ§ª Testing Semantic Search Engine\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Test with the first sample question\n",
        "    question = sample_questions[0]\n",
        "    print(f\"\\nğŸ“ Sample Question: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        result = search_engine.ask_question(question)\n",
        "        \n",
        "        print(f\"\\nğŸ’¡ Answer:\")\n",
        "        print(f\"{result['answer']}\")\n",
        "        \n",
        "        print(f\"\\nğŸ“š Source Documents:\")\n",
        "        for i, doc in enumerate(result['source_documents'], 1):\n",
        "            print(f\"   {i}. {doc.metadata.get('source', 'Unknown')} (Preview: {doc.page_content[:100]}...)\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during question answering: {str(e)}\")\n",
        "    \n",
        "    print(f\"\\nğŸ’¡ Try other questions:\")\n",
        "    for i, q in enumerate(sample_questions[1:], 2):\n",
        "        print(f\"   {i}. {q}\")\n",
        "\n",
        "# Run the test\n",
        "test_search_engine()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interactive Question Answering\n",
        "\n",
        "Use this cell to ask your own questions about the documents:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Interactive question function ready!\n",
            "Usage: ask_custom_question('Your question about the documents')\n"
          ]
        }
      ],
      "source": [
        "# Interactive question answering\n",
        "def ask_custom_question(question: str):\n",
        "    \"\"\"Ask a custom question to your documents.\"\"\"\n",
        "    \n",
        "    if not search_engine.qa_chain:\n",
        "        print(\"âŒ QA chain not available. Please run the setup cells first.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"ğŸ¤” Your Question: {question}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    try:\n",
        "        result = search_engine.ask_question(question)\n",
        "        \n",
        "        print(f\"\\nğŸ’¡ Answer:\")\n",
        "        print(f\"{result['answer']}\")\n",
        "        \n",
        "        print(f\"\\nğŸ“š Sources:\")\n",
        "        for i, doc in enumerate(result['source_documents'], 1):\n",
        "            source = doc.metadata.get('source', 'Unknown')\n",
        "            preview = doc.page_content[:150].replace('\\n', ' ')\n",
        "            print(f\"   {i}. {source}\")\n",
        "            print(f\"      Preview: {preview}...\")\n",
        "            \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# result = ask_custom_question(\"Your question here\")\n",
        "\n",
        "print(\"âœ… Interactive question function ready!\")\n",
        "print(\"Usage: ask_custom_question('Your question about the documents')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Search Similar Documents\n",
        "\n",
        "You can also search for documents similar to a query without asking questions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Document search function ready!\n",
            "Usage: search_similar_documents('your search query', k=3)\n"
          ]
        }
      ],
      "source": [
        "# Search for similar documents\n",
        "def search_similar_documents(query: str, k: int = 3):\n",
        "    \"\"\"Search for documents similar to a query.\"\"\"\n",
        "    \n",
        "    if not search_engine.vector_store:\n",
        "        print(\"âŒ Vector store not available. Please run the setup cells first.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"ğŸ” Searching for: '{query}'\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    try:\n",
        "        results = search_engine.search_documents(query, k=k)\n",
        "        \n",
        "        print(f\"\\nğŸ“„ Found {len(results)} similar documents:\")\n",
        "        \n",
        "        for i, doc in enumerate(results, 1):\n",
        "            source = doc.metadata.get('source', 'Unknown')\n",
        "            content_preview = doc.page_content[:200].replace('\\n', ' ')\n",
        "            \n",
        "            print(f\"\\n{i}. Source: {source}\")\n",
        "            print(f\"   Content Preview: {content_preview}...\")\n",
        "            print(f\"   Length: {len(doc.page_content)} characters\")\n",
        "            \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during search: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# results = search_similar_documents(\"your search query\", k=3)\n",
        "\n",
        "print(\"âœ… Document search function ready!\")\n",
        "print(\"Usage: search_similar_documents('your search query', k=3)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ Congratulations!\n",
        "\n",
        "You've successfully built a semantic search engine with the following features:\n",
        "\n",
        "### âœ… What You've Built:\n",
        "- **Document Loading**: Automatically loads PDF, DOCX, and TXT files from GCS bucket\n",
        "- **Text Processing**: Intelligently splits documents into searchable chunks\n",
        "- **Vector Search**: Uses advanced embeddings for semantic similarity search\n",
        "- **AI Question Answering**: Powered by Gemini 2.5 Pro for natural language responses\n",
        "- **Source Attribution**: Shows which documents were used to answer questions\n",
        "\n",
        "### ğŸš€ Next Steps:\n",
        "1. **Add More Documents**: Upload more files to your GCS bucket and re-run the setup\n",
        "2. **Customize Parameters**: Adjust chunk size, embedding models, or search parameters in the config\n",
        "3. **Advanced Features**: Add document filtering, different prompt templates, or conversation memory\n",
        "4. **UI Integration**: Build a web interface using Streamlit or Gradio\n",
        "5. **Production Setup**: Add error handling, logging, and monitoring\n",
        "\n",
        "### ğŸ› ï¸ Configuration Summary:\n",
        "- **GCS Bucket**: `{config.GCS_BUCKET_NAME}`\n",
        "- **Model**: `{config.MODEL_NAME}`\n",
        "- **Embedding Model**: `{config.EMBEDDING_MODEL}`\n",
        "- **Chunk Size**: `{config.CHUNK_SIZE} characters`\n",
        "\n",
        "### ğŸ“ Usage Examples:\n",
        "```python\n",
        "# Ask questions about your documents\n",
        "result = ask_custom_question(\"What are the main findings?\")\n",
        "\n",
        "# Search for similar content\n",
        "docs = search_similar_documents(\"machine learning applications\")\n",
        "```\n",
        "\n",
        "Happy searching! ğŸ”âœ¨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
